{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml_parse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yaml_parse.fill_gaps()\n",
    "# yaml_parse.save_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The data consists of 10 observations. Bla, Bla, ...."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "# Instead of setting the cell to Markdown, create Markdown from withnin a code cell!\n",
    "# We can just use python variable replacement syntax to make the text dynamic\n",
    "n = 10\n",
    "md(\"The data consists of {} observations. Bla, Bla, ....\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 5, 1, 18, 43, 50, 637042)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "delt = datetime.timedelta(days=2)\n",
    "delt\n",
    "now - delt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no updates for  Data Visualization Roadshow With Jeff Oliver\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Data & Visualization Weekly Projects Report 2021_04_30\n",
       "\n",
       "## Active Projects \n",
       "\n",
       "\n",
       "### Active Development \n",
       "\n",
       "* Advice For Thesis Defense Visualizations, Sabrina Nardin \n",
       "\t * Answered follow up questions about her new stream graph over email\n",
       "* Biosphere 2 Biosystems Visualization Collaboration \n",
       "\t * Didn't connect with Omani last week because of a time conflict on her end with building maintenance\n",
       "\t * Started working on a Proof of Concept for helping to convert models into point clouds\n",
       "\t * Wrote a js triangle sampling program which will be used when loading pixels from an image that is used as a mesh face texture. https://trianglesampler.baylyd.repl.co/\n",
       "\t * Had a big meeting with Ash, Victoria, and  Aaron and it sounds like I'll be working with Tech Core students this summer on the Biosphere2 collab\n",
       "* Bryan Carter Photogrammetry \n",
       "\t * Meeting Tayvien who works with Bryan on Thursday this week\n",
       "\t * Will spend some time getting him started with apporto and metashape to test things out\n",
       "* Data Visualization Roadshow With Jeff Oliver \n",
       "* Independent Study Abby Collier \n",
       "\t * Abby won the SBS senior award !!\n",
       "\t * Met and discussed how to use actively moving particles positions for sampling other textures\n",
       "\t * This is the foundation of reactive particle systems, so we now can use video input of a person to manipulate our particle system!\n",
       "\t * Will work with Blake to submit grade before the end of the semester\n",
       "* Jason Hortin Holographic Dance Graduate Project \n",
       "\t * Wrote another email to Jason and mentioned he should get his project into the running for projects listed in the tech core summer Internship\n",
       "\t * He also reached out to Dr. Bryan Carter recently to see what the options were for doing volumetric capture footage recording\n",
       "\t * Sounds like more will happen in the fall because he's going back to Chicago for the summer\n",
       "* Judging The Data Visualization Challenge \n",
       "\t * Completed this, my favorite entries also were the ones that won so at least I was judging along the lines of the others\n",
       "* Migrant Forensic Empathy Project: A Digital Borderlands Grant Initiative \n",
       "\t * Time to really push on this project\n",
       "\t * discovered that the \"shader:flat\" method can help eliminate artifacts introduced from a renderer that's struggling with shadows on a large mesh\n",
       "\t * rewrote the populate and query side of the quad-tree in non recursive algorithms, and dealt with bug that was running my machine out of memory in the development of this\n",
       "\t * New ideas about how to integrate the shrine into the landscape in a modular fashion\n",
       "\t * Will probably make a shallow quadtree for querying the user position every 5 seconds to see if we need to render more or less crosses in the environment\n",
       "* Oyster Vibrio Literature Review \n",
       "\t * Emily had her defense last week, will now reach out to see if she is happy with the project and wants to call it done\n",
       "* Remote Visualization Infrastructure Development \n",
       "\t * Met with Chris and discussed remote visualization options\n",
       "\t * Sounds like we should look into other tools that support rendering vulkan windows that aren't just NoMachine, still need to get to the bottom of that conversation with Chris\n",
       "\t * Discussed Omniverse usage\n",
       "\t * Tested Vulkan on each HPC and each have the same Invalid Driver message\n",
       "\t * Created a vulkan issue on their tracker https://vulkan.lunarg.com/issue/view/608706d05df112e5e0a11a5a\n",
       "* Resbaz Organizer And Workshop Provider \n",
       "\t * Met Chinmay one more time\n",
       "\t * Heard back from Kelsey about what the listing should be for the videos (public unless the instructor says otherwise), I think we need to make sure that's spelled out for folks attending workshops\n",
       "\t * Got our Observable workshop listed in the webpage\n",
       "* Stellarscape Astronomy Multimedia Dance Performance \n",
       "\t * Wrote code to generate multisegment lines, but got stuck with how to apply this with multi line data also\n",
       "\t * Got curious about the scanline fill algorithms used to color the interior of geometries that we express with lines\n",
       "\t * Met with Gustavo from the sensor lab to discuss our options.\n",
       "\n",
       "\n",
       "### Consultations \n",
       "\n",
       "* Advice For Thesis Defense Visualizations, Sabrina Nardin \n",
       "\t * Answered follow up questions about her new stream graph over email\n",
       "* Bryan Carter Photogrammetry \n",
       "\t * Meeting Tayvien who works with Bryan on Thursday this week\n",
       "\t * Will spend some time getting him started with apporto and metashape to test things out\n",
       "* Jason Hortin Holographic Dance Graduate Project \n",
       "\t * Wrote another email to Jason and mentioned he should get his project into the running for projects listed in the tech core summer Internship\n",
       "\t * He also reached out to Dr. Bryan Carter recently to see what the options were for doing volumetric capture footage recording\n",
       "\t * Sounds like more will happen in the fall because he's going back to Chicago for the summer\n",
       "\n",
       "## Upcoming \n",
       "\n",
       "* Collaboration With Techcore's Summer Internship \n",
       "* Has Faculty Collaborations With Holodeck \n",
       "* Observablehq Portfolio Of Data Visualization \n",
       "* Radiology 1St Year Resident Carl Sabotke \n",
       "* Ray Tracing On The Hpc \n",
       "\n",
       "\n",
       "## Completed For Fiscal Year \n",
       "\n",
       "\n",
       "\n",
       "### Workshops/Trainings \n",
       "\n",
       "* Mt. Lemmon In Your Pocket-Creating A Virtual Reality Tour \n",
       "\t * https://rtdatavis.github.io/#GIS_week2020 \n",
       "* Presentation For Civil Engineering Department \n",
       "\t * https://docs.google.com/presentation/d/15Z9zcxU4vIIgFPnKEcaGv9GH7JtjNdx4Xpnjec0EzEc/edit?usp=sharing \n",
       "* Tech Core Level Up Presentation Monday, Sept 28 2020 \n",
       "\t * https://rtdatavis.github.io/#techcoresept28 \n",
       "* Tech Core Level Up Presentation Tuesday, Mar 17 2020 \n",
       "\t * https://rtdatavis.github.io/#techcoremar20 \n",
       "* Womens Hackathon: Visualization On The Web Workshop \n",
       "\t * https://womenshackathon.arizona.edu/ \n",
       "\t * https://www.youtube.com/channel/UCe1YiJ53o3qcayVs4cipeXA/videos \n",
       "\t * https://www.youtube.com/watch?v=VLwPOtqW8oM \n",
       "\n",
       "\n",
       "### Completed Projects/Collaborations \n",
       "\n",
       "* 3D & Vr Retrofit Azlive \n",
       "\t * https://rtdatavis.github.io/#retrofitAZLIVE \n",
       "* Bio5 Virtual Reality Tour \n",
       "\t * https://rtdatavis.github.io/#bio5-vr-tour \n",
       "* Covid Retail Mitigation Web Scraping \n",
       "\t * https://rtdatavis.github.io/#retailscraping \n",
       "* Force Directed Biochem Networks \n",
       "\t * https://rtdatavis.github.io/#biochem-networks \n",
       "* Neuro Choropleth \n",
       "\t * https://rtdatavis.github.io/#neuro-choro \n",
       "* Spring Break Covid Photo Maps \n",
       "\t * https://rtdatavis.github.io/#spring-break-covid \n",
       "\n",
       "\n",
       "### Infrastructure Developed \n",
       "\n",
       "* Autamus Web Interface \n",
       "\t * https://rtdatavis.github.io/#autamus_interface \n",
       "* Virtualgl For Nvidia Accelerated Remote Hpc Visualizations \n",
       "\t * https://rtdatavis.github.io/#virtualgl \n",
       "* Xpra And Singularity For Comprehensive Graphical Application Support On Hpc \n",
       "\t * https://rtdatavis.github.io/#xprasingularity \n",
       "\n",
       "\n",
       "### Protocols and Analysis Developed \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import yaml\n",
    "from io import StringIO\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def run():\n",
    "    now = datetime.datetime.now() - datetime.timedelta(days=3)\n",
    "    \n",
    "    with open(\"first_doc.yaml\",\"r\") as phile:\n",
    "        # projects\n",
    "        p = yaml.full_load(phile)\n",
    "    ophile = StringIO()\n",
    "    dc = p[\"detailed_collection\"]\n",
    "    ophile.write(f\"# Data & Visualization Weekly Projects Report {now:%Y}_{now:%m}_{now:%d}\\n\")\n",
    "    ophile.write(f\"\\n## Active Projects \\n\\n\")\n",
    "    ophile.write(f\"\\n### Active Development \\n\\n\")\n",
    "    for p_name_key in dc:\n",
    "        deets = dc[p_name_key]\n",
    "        #print(deets[\"status\"])\n",
    "        if deets[\"status\"] == \"active\":\n",
    "            ##print(\"details\",p_name_key,deets,\"\\n\\n\")\n",
    "            ophile.write(f\"* {p_name_key} \\n\")\n",
    "            ## go through the under points\n",
    "            ## this will be a nested list\n",
    "            #print(deets)\n",
    "            if deets[\"updates\"] != None and len(deets[\"updates\"]) > 0:\n",
    "                latest = deets[\"updates\"][0]\n",
    "                if not deets[\"newUpdates\"]:\n",
    "                    print(\"no updates for \",p_name_key)\n",
    "                    continue\n",
    "                for update in latest:\n",
    "                    ophile.write(f\"\\t * {update}\\n\")\n",
    "    ophile.write(f\"\\n\\n### Consultations \\n\\n\")\n",
    "    for p_name_key in dc:\n",
    "        deets = dc[p_name_key]\n",
    "        if deets[\"type\"] == \"consult\" and deets[\"status\"]== \"active\":\n",
    "            ##print(\"details\",p_name_key,deets,\"\\n\\n\")\n",
    "            ophile.write(f\"* {p_name_key} \\n\")\n",
    "            ## go through the under points\n",
    "            ## this will be a nested list\n",
    "            #print(deets)\n",
    "            if deets[\"updates\"] != None and len(deets[\"updates\"]) > 0:\n",
    "                latest = deets[\"updates\"][0]\n",
    "                if not deets[\"newUpdates\"]:\n",
    "                    print(\"no updates for \",p_name_key)\n",
    "                    continue\n",
    "                for update in latest:\n",
    "                    ophile.write(f\"\\t * {update}\\n\")\n",
    "    ophile.write(f\"\\n## Upcoming \\n\\n\")\n",
    "    for p_name_key in dc:\n",
    "        deets = dc[p_name_key]\n",
    "        if deets[\"status\"] == \"upcoming\":\n",
    "            ophile.write(f\"* {p_name_key} \\n\")\n",
    "    ophile.write(f\"\\n\\n## Completed For Fiscal Year \\n\\n\")\n",
    "    ## sections for this\n",
    "    # workshop section\n",
    "    ophile.write(f\"\\n\\n### Workshops/Trainings \\n\\n\")\n",
    "    section_fill(dc,ophile,\"complete\",\"workshop\")\n",
    "    # collabs\n",
    "    ophile.write(f\"\\n\\n### Completed Projects/Collaborations \\n\\n\")\n",
    "    section_fill(dc,ophile,\"complete\",\"collaboration\")\n",
    "    #infra\n",
    "    ophile.write(f\"\\n\\n### Infrastructure Developed \\n\\n\")\n",
    "    section_fill(dc,ophile,\"complete\",\"infrastructure\")\n",
    "    #protocols and analysis\n",
    "    ophile.write(f\"\\n\\n### Protocols and Analysis Developed \\n\\n\")\n",
    "    return ophile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def section_fill(dc,ophile,status,typ):\n",
    "    for p_name_key in dc:\n",
    "        deets = dc[p_name_key]\n",
    "        if deets[\"status\"] == status and deets[\"type\"] == typ:\n",
    "            ophile.write(f\"* {p_name_key} \\n\")\n",
    "            #print(deets)\n",
    "            if status == \"complete\":\n",
    "                links = deets['links']\n",
    "                for link in links:\n",
    "                    ophile.write(f\"\\t * {link} \\n\")\n",
    "\n",
    "clear_output(wait=True)\n",
    "result = run()\n",
    "result.seek(0)\n",
    "md(result.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.seek(0)\n",
    "now = datetime.datetime.now() - datetime.timedelta(days=3)\n",
    "with open(f\"{now:%Y}-{now:%m}-{now:%d}-Data-Vis-Weekly.md\",\"w\") as phile:\n",
    "    phile.write(result.read())\n",
    "with open(\"convert_temp.md\",\"w\") as phile:\n",
    "    phile.write(result.read())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import shutil\n",
    "#import python_quick\n",
    "\n",
    "with open('first_doc.yaml','r') as phile:\n",
    "    contents = yaml.full_load(phile)\n",
    "\n",
    "def fill_gaps():\n",
    "    ## this is intended to add other elements to the detailed_collection section\n",
    "    dc  = contents['detailed_collection']\n",
    "    attrs = contents['attributes']\n",
    "    for name in contents['names']:\n",
    "        dc_ele = dc.get(name.title(),-1)\n",
    "        ## check whether we have a project by this name yet\n",
    "        if dc_ele != -1:\n",
    "        ## this would be where we make sure all the attributes are present\n",
    "            for attr in attrs:\n",
    "                if dc_ele.get(attr,-1) == -1:\n",
    "                    dc_ele[attr] = None\n",
    "        ## if not \n",
    "\n",
    "        if dc_ele == -1:\n",
    "            dc[name.title()]={attr:None for attr in contents[\"attributes\"]}\n",
    "\n",
    "\n",
    "    \n",
    "def save_contents() :\n",
    "    backups = [f for f in os.listdir() if \"backup\" in f]\n",
    "    with open(\"first_doc.yaml\",\"w\") as phile:\n",
    "        yaml.dump(contents,phile)\n",
    "    ## this gives us actual empty strings for ease\n",
    "    with open(\"first_doc.yaml\",\"r\") as phile:\n",
    "        text_with_nones = phile.read()\n",
    "    correct_text = text_with_nones.replace(\"null\",\"\")\n",
    "    shutil.copy(\"first_doc.yaml\",f\"first_doc_backup_{len(backups) +1}.yaml\")\n",
    "    with open(\"first_doc.yaml\",\"w\") as phile:\n",
    "        phile.write(correct_text)\n",
    "\n",
    "def build_requests_from_yaml():\n",
    "    # pen the yaml\n",
    "    with open(\"first_doc.yaml\",\"r\") as phile:\n",
    "        structured_doc = yaml.full_load(phile)\n",
    "    # create a string\n",
    "    result_string = \"\"\n",
    "    # parse the yaml\n",
    "    dc =  structured_doc[\"detailed_collection\"]\n",
    "    for project_entry in dc:\n",
    "        result_string += f\"*{project_entry}*\\n\\n\"\n",
    "        pe = dc[project_entry]\n",
    "        ## go over all the individual attributes of project and add that to the result string\n",
    "        for attr in pe:\n",
    "    # add things with the correct headers to the string\n",
    "            result_string += f\"---{attr}---\\n\\n{pe[attr]}\\n\\n\"\n",
    "\n",
    "\n",
    "    # then return\n",
    "    return form_request(result_string)\n",
    "\n",
    "def form_request(text):\n",
    "    requests = [\n",
    "         {\n",
    "            'insertText': {\n",
    "                'location': {\n",
    "                    'index': 1,\n",
    "                },\n",
    "                'text':text \n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    return requests\n",
    "\n",
    "\n",
    "\n",
    "def send_to_gdocs(doc_id,requests):\n",
    "    service = python_quick.build_service()\n",
    "    service.documents().batchUpdate(documentId=doc_id,body={'requests':requests}).execute()\n",
    "\n",
    "\n",
    "def full_run():\n",
    "    fill_gaps()\n",
    "    save_contents()\n",
    "    #document_id =\"1HLsxu3P6C43GiS3qCAOgv3ZWZcN7Tept4oSdOdR2lIU\" \n",
    "    #req = build_requests_from_yaml()\n",
    "    #send_to_gdocs(document_id,req)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_gaps()\n",
    "save_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
