attributes:
- type
- status
- description
- updates
- dates
- contacts
- newUpdates
- links
names:
- Streaming technology for HPC
- Vulkan on HPC
- Virtual Nature
- Stellarscape Astronomy Multimedia Dance Performance
- 'Migrant Forensic Empathy project: a Digital Borderlands grant initiative'
- Independent study Abby Collier
- Presentation for Civil Engineering Department
- Data Visualization Roadshow with Jeff Oliver and Kiri Carini
- Judging the data visualization challenge
- Remote visualization infrastructure Development spring 2021
- Oyster Vibrio Literature Review
- Ray tracing on the HPC
- HAS Faculty collaborations with Holodeck
- COVID Retail Mitigation Web Scraping
- Autamus web interface
- Bryan Carter Photogrammetry
- 'Womens Hackathon: Visualization on the web workshop'
- ObservableHQ portfolio of Data Visualization
- Jason Hortin holographic dance graduate project
- Advice for thesis defense visualizations, Sabrina Nardin
- Collaboration with TechCore's Summer Internship
- Resbaz organizer and workshop provider
- force directed biochem networks
- neuro choropleth
- spring break COVID photo maps
- VirtualGL for Nvidia accelerated remote HPC visualizations
- Xpra and Singularity for comprehensive Graphical application support on HPC
- 3D & VR Retrofit AZLIVE
- BIO5 virtual reality tour
- social VR museum capstone student project
- Tech core level up presentation Tuesday, Mar 17 2020
- Tech core level up presentation Monday, Sept 28 2020
- Argonne GPU hackathon
- Radiology 1st year resident Carl Sabotke
- Mt. Lemmon in your Pocket-creating a virtual reality tour
- Meeting with Tyson Swetnam to discuss containers and remote visualization
- IEEE Satellite Vis Conference Planning
- Thermal Imaging Project
- CATalyst data studio vis wall playlist 
- Volumetric Capture processing on HPC
- TURN UP Festival Performance NYU/UA collab
detailed_collection:
  - - Template
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: true
      status: active
      type: collaboration
      updates:
      - - None
  - - Lilliana  Salas GIS GIDP
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: true
      status: active
      type: consult
      updates:
      - - Lilliana is trying to gather her data on alternative transport mechanisms in the city of Austin
        - I've been helping her collect data for her publication reviewers and introducing the google colab python notebooks for reproducible science
  - - Laura Miller Math
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: true
      status: active
      type: consult
      updates:
      - - Laura has some really neat 3d flow data of what happens in a fluid when a JellyFish pulses and I'm trying to help her create some non vtk/visit visualizations
  - - Kirsten Ball ENVS
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: true
      status: active
      type: consult
      updates:
      - - Worked with Kirsten's Soil Data to try to produce parallel coordinates plot of different microbe community percentages under the different categories of Irrigation, and Depth of the sample
  - - Arminda Estrada, ECE
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: true
      status: active
      type: consult
      updates:
      - - Arminda and I worked on trying to get her machine to remotely display the ROS open source robotics program with Nvidia Hardware Acceleration
  - - TURN UP Festival Performance NYU/UA collab
    - contacts: Win Burleson, Kay He
      dates: null
      description: null
      links: null
      newUpdates: true
      status: upcoming
      type: collaboration
      updates:
      - - None
  - - Volumetric Capture processing on HPC
    - contacts: Bryan Carter
      dates: null
      description: null
      links: null
      newUpdates: true
      status: upcoming
      type: collaboration
      updates:
      - - None
  - - Has Faculty Collaborations With Holodeck
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: false
      status: upcoming
      type: collaboration
      updates: null
  - - Ray Tracing On The Hpc
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: false
      status: upcoming
      type: collaboration
      updates: null
  - - Observablehq Portfolio Of Data Visualization
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: false
      status: upcoming
      type: community
      updates: null
  - - CATalyst data studio vis wall playlist
    - contacts: Jen Nichols
      dates: null
      description: null
      links: null
      newUpdates: true
      status: active
      type: Infrastructure
      updates:
      - - Setup meeting with Jen for next week
        - Goal of short project is to create a web page that will be screen saver for the big vis wall at CATalyst
        - Have process that iterates over videos and live demos of visualizations and creative code works
        - Make it something others can add to over time
        - Should be a great way to be in the driver's seat for what material gets viewed by people visiting the space
        - This will make it easy to have a showcase option for RT's work
  - - Thermal Imaging Project
    - contacts: Ed Wellman, Brad Ross
      dates: null
      description: null
      links: null
      newUpdates: true
      status: active
      type: collaboration
      updates:
      - - Had first meeting with Brad Ross and Ed Wellman
        - Received their project directory to explore and several documents detailing the previous steps of the project
        - Sounds like they want someone to explore a few different types of motion detection algorithms by January so they can put that information into next milestone document
        - If possible would like to make the workflow run on the HPC so that they can process videos in bulk
        - Set monday as date to meet and go over material with Ed
  - - IEEE Satellite Vis Conference Planning
    - contacts: null
      dates: null
      description: null
      links: 
      - https://www.appsheet.com/start/7e25f1d2-e9a4-4eec-a31a-5edd275f3059
      newUpdates: true
      status: active
      type: collaboration
      updates:
      - - Created self updating google form for the Birds of a Feather submission
        - Organized connection for producing the Satellite conference promotional video
        - Attended meeting with videographer and Josh 
        - Started thinking about what live streams to watch during the satellite event
        - Reached out to folks who might be interested in showing their services at the Showcase Lunch on Tuesday
      - - Had meeting on Friday planning video shoutout material
        - Decided to start emailing folks in the stakeholder list I created about the conference that's coming up
        - Rooms and Birds of a Feather timing worked out
        - Had meeting with DSRT folks recently and discussed the conference, folks on panel are going to help advertise
      - - Spent 2 hours in meeting working out details related to the location reservations for the conference as well as community planning steps
        - Volunteered to create the Birds of a Feather (BOF)  sign up sheet with Google's AppSheet https://www.appsheet.com/start/7e25f1d2-e9a4-4eec-a31a-5edd275f3059
        - Also volunteered to be the main person at the information table during the conference helping folks identify which vis experts their questions are best suited for
  - - Vulkan on HPC
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: true
      status: active
      type: infrastructure
      updates:
      - - Tested out using OOD with the boids flocking and it works
        - Looked into running rust nannou creative coding library and that still throws the "adapter not found" error
        - Isoloated the glib version mismatch to the `shaderc` library dependency, will investigate `Naga` or `glsllang` to get around this
        - Next step is to take the locally running HDF5 renderer and test it on ocelote 
      - - In the process of creating a program that will read from hdf5 files and transparently draw points to image texture using GPU to be saved out as individual frames of Astronomy Simulation videos
      - - Reading section on windowless rendering from https://sotrh.github.io/learn-wgpu/showcase/windowless/
        - Hopefully this will make it possible to run simulations steps faster than the x11 can forward and then stitch the png results together in a movie file
      - - Got help from Adam and Ric last week and was able to have the wgpu boids simulation running on the HPC sending frames to laptop with x11 forwarding
        - Adam also put the vulkan sdk into a module so that we could use it outside of r5u09n1 the puma node that he had set aside for testing
        - Started running into new errors `No valid $DISPLAY found. Unable to load module.` when trying to do other work on the HPC now though
      - - Vulkan is the successor to OpenGL and much of my low level graphics understanding comes from web-gpu that leverages Vulkan
        - This means in order to provide greater visualization support with the HPC I need to have access to Vulkan
        - The issues are that our nvidia driver setup has a bug in it related to Vulkan support
        - I tried to get around this by configuring a singularity container with the files we know to be missing but a new error arises 600 lines later in the `vulkaninfo` command now
        - Reached out to UCSD and Expanse HPC visualization personnel to inquire about their support for this modern graphics library
        - Sent email to the Campus Champions list serve about this but didn't get many useful replies
  - - Streaming Technology for HPC
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: false
      status: active
      type: infrastructure
      updates:
      - - Spent time working out how to use FFMPEG to send frames from laptop webcam over udp as concatenated JPEG to an intermediary server that received upd and converted that to http Transfer Encoded Chunks received in a web browser and displayed as an updating image
        - This will eventually develop into a program that can take researcher jpg udp frames and display them where ever the researcher wants to display them as long as the display client is also on the HPC VPN
        - This is a simpler remote visualization method than use desktops or VNC because it only requires running a script on the HPC, but does mean a researcher has to write code to send the images over udp
        - Either way, it should be less work for the Infrastructure team and is a simpler remote client setup which just has to be a browser page
  - - Data Visualization Roadshow With Jeff Oliver and Kiri Carini
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: true
      status: active
      type: collaboration
      updates:
      - - Met last week and discussed upcoming presentations 
        - First is Research Training Group for Mathematics, we will be using Jellyfish swimming fluid dynamics data, the big task is simplifying some of this to be used
        - Next is the Special Library Association presentation, we will be explaining to other librarians our approach to explaining data visualization for different groups around campus
      - - Met and revised presentation with Jeff Oliver and Kiri
        - Worked on tool for Kirsten Ball as individual consultation
      - - Setup two new presentations for the fall so the remaining total is 5
        - First new presentation is for Special Libraries Association (SLA) and their Science Technology Community sub group
        - Second is the Math department Research Training Group (RTG) with Laura Miller and Kevin Lin
        - Had a debrief session from last presentation and we are  going to do a bottom to top overhaul of the presentation for the ENVS presentation on the 27th
      - - Had first drop in last week, spent time with Kiri figuring out how the semester might look
        - Planned out some presentations at the GIS day in November also
        - Aiming to create demonstration of streaming point cloud gdal stuff from HPC the same way the USGS uses Entwine for it's national Geospatial data
      - - Created new flyer for the catalyst studio presentation on the 2nd of Sept
        - Kiri volunteered to reserve our space at the studios
        - Worked on material that needs to be swapped out from the presentation template
        - Prepared material for first drop in time next Tuesday
      - - Planning the catalyst studios presentation that is our first of 6 "Intro to Data Vis" presentations for this fall
        - The following presentations will happen and others may be added (ENVS, Math, Level up Eller, 2nd Catalyst presentation, Public Health) 
      - - Reviewed form for collaborative drop ins with myself Jeff Oliver and Kiri
          Carini at Catalyst
        - Received ENVS seminar data from Kirsten Ball
        - Set presentation date for ENVS on sept 27th at 3pm
      - - Met with Jeff and planned the changes we have to make to the presentation
          for next thursday
        - I decided on the dataset that I'll be using in my part of the presentation
          (SIR model for virus transmission)
        - Sent messages out to Catalyst to figure out a meeting time for presentations
          there in the fall
        - Jeff invited me to attend the Software carpentry instructor training happening
          this fall and I think that sounds like a great idea
      - - Set Thursday next week for  meeting to prepare for first presentation on
          the 15th for Laura Miller's Research Training Group in the Math Department
      - - Reached out to Catalyst studios for scheduling our kickoff and wrapup data
          visualization presentations for general audiences
        - Jeff has been in contact with the college specific librarians at Eller and
          Public Health setting up presentations
        - We've already scheduled 2 presentations with another 6 in the works, and
          then I think we will only add more upon request
      - - Flurry of emails week
        - created google sheet to track who's emailing who and why and what the contact's
          affiliation is
        - Sent out close to 10 separate emails which each made several more contacts
        - Set the constraint on performing a minimum of 7 presentations in the fall
      - - Finished writing stock emails for sending out to faculty we believe are
          teaching graduate seminars, and the coordinators who can tell us who else
          is teaching seminars
      - - Met Jeff and discussed approaches for presentations next fall
        - Started creating stock email offer for presentation to send to assembled
          list of contacts
        - Will perhaps target graduate seminars and section leader/TA groups which
          have more availability than an undergraduate class
      - - Setup meeting with Jeff next week to plan our approach to graduate intro
          to vis presentations for the fall
      - - Heard back from the NSCS department about a presentation
        - Just working on the date for the presentation
        - Presenting to undergraduates also
      - - Reaching out to the NSCS department to ask whether they have a graduate
          seminar which we can present to?
      - - Sent offer for presentation to Dianne Patterson in SLHS
        - She isn't able to offer actual class time for a presentation
        - Will have to decide with Jeff if we really don't want to pre-record something
          for them
        - Trying the NSCS department next
  - - Independent Study Abby Collier
    - contacts: null
      dates: null
      description: null
      links:
      - https://openprocessing.org/user/255658?view=sketches
      - https://observablehq.com/@aecollier/sqrrules?ui=classic
      - https://aecollier.github.io/portfolio/
      newUpdates: true
      status: active
      type: student
      updates:
      - - Met with Abby and discussed show running techniques
        - This involves taking a step back and figuring out how we are planning to trigger start, end, transition behaviors in the effects that we've generated so far
        - Much more python code getting used at this point than before
        - Getting her support deciding how to make effects transition between their initialization and running states
      - - Met with Abby and talked about Github actions for automated cloud operations
        - Switched to working on stream disruption effects 
        - Discussed a little bit of the Kalman filter bayesian questions that I have
      - - Met with Abby and worked on the 3rd effect from the performance's movement 1a
        - Left it on a note where I described 5 modifications of varying difficulty that she could investigate 
      - - Met to work on combining effects for the 1a movement's motion aura effect
        - This included teaching her about glsl atomic counters
        - implemented system by which she can work on the same Touch Designer files as me and labeled various parts of Kay's feedback by difficulty level to make it easier for her to assist me
      - - Met briefly last week to cover recent updates
        - Discussed plan for distributing the work related to the feedback given from Kay in the last meeting
      - - Met with Abby and discussed how to use the multithreading concept of "atomic"
          counters for use in compute shaders
        - worked on several effects where the particles that are generated come off
          of the dancer and move in the direction that the dancer was proceeding when
          the particles were born.
      - - Met with Abby and worked on compute shader stuff for self advection of color
        - we also worked on noise generation inputs for the astro dance performance
          movement 2c
      - - Met with Abby and worked on teaching her the fundamentals of rendering lines
          from glsl custom materials and line primitives
        - Gave her some instructions about updating the example touch designer network
          that we are working with
      - - Met with Abby and worked on numpy boolean indexing techniques to apply to
          masked grid effects for the astro dance project visuals
        - Spent the rest of the time revisiting various ideas of how to create touch
          designer networks for visualizations
      - - Met and setup access to a small jetstream machine for learning more about
          docker and containerization
        - Spent more time attempting a vectorization of lines following all-to-all
          connection pattern in a circle, but got stuck on numpy errors
        - switched to just doing this in touch designer within a single glsl top
        - ended meeting with idea that we will just  be making a grid and then filtering
          based on a radius of a given size to control what points are shown
        - really need to get back to the TCP stuff for transfering data to active
          touch sessions
      - - Met and discussed creating regularly spaced data in numpy using vectorize
          that can be the backbone of touch designer visualizations
      - - Continuing this connection as part of working on the Astro Dance project
        - Met up and discussed using docker containers
        - Shared "Play with Docker" tool to practice docker
        - Showed her how to use ssh to connect with PuTTY
        - Sent data from remote machine over ssh to Touch Designer using tcp
        - Need to refocus on how these techniques are going to help us going forward
      - - Met up and started covering TCP protocol for transmitting images from JAX/numpy
          server into touch designer
        - This helps us extend to creating assets outside of the touch designer system,
          and offload larger tasks to more performant machines
        - Covered some more information about drawing grids with compute shaders,
          and the numpy Script TOP
      - - Spent a good amount of time at the begininning looking through materials
          provided by Kay He to see which effect she would like to work on next
        - Settled on her helping out with the monocolor circular lines
        - Met and started working on compute shader code involving grids
        - Sent off Abby's portfolio to Holly Brown https://aecollier.github.io/portfolio/
      - - Met and added final touches to her feature piece for the final part of the
          independent study
        - Submitted grade with Blake, and will send Holly Brown a brief report with
          Abby's independent study webpage
      - - Abby won the SBS senior award !!
        - Met and discussed how to use actively moving particles positions for sampling
          other textures
        - This is the foundation of reactive particle systems, so we now can use video
          input of a person to manipulate our particle system!
        - Will work with Blake to submit grade before the end of the semester
      - - Had our usual friday meeting
        - Made quite a bit of progress on her feature Touch Designer piece called
          "purple stars"
        - Spent time working on her github webpage to present the work from this independent
          study
        - Talked about potential for her to continue in this capacity after graduation
          as a DCC
      - - Met up and started working on custom materials
        - Had long discussion in the meeting cementing  the mental model of how the
          shaders are using different color channels to move particles
        - Also started working on how to use other texture inputs to help with particle
          spreading patterns
        - Had to rush into using uniform sampler2D types
        - As a result I made a short video recording explaining how to do modular
          arithmetic to calculate texture sample coordinates from instance id's  for
          each of our particles
      - - Met with Abby several times this week
        - The main focus of our time is on how to get her help with the touch designer
          visualizations for the show
        - Kay has requested a recreation of this effect https://www.youtube.com/watch?v=r9dd6csVZbk
          and I'm making it Abby's focus
        - Discussed feedback loops, texture lookups, neighbor queries, and soon we
          will talk about writing custom materials with shader code
      - - Met with Abby and started working on Touch designer
        - Wrote a section of Abby's outstanding senior nomination for Rich Thompson
          who is the primary Python lecturer for ISTA
        - Worked live through a Touch Designer network showing basic features such
          as CHOPs TOPs, parameter referencing, and feedback
        - Then did a very quick intro to programming particle systems as that will
          be the main thing we generate for the astro dance performance
      - - Met on friday to discuss final tweaks to the data visualization feature
          piece
        - Abby built a very nice heatmap displaying squirrel counts from the data
          set against the hectares they live in within central park
        - Worked on making legend for her graph, and how to add observational notes
          to the data
        - She will add some prose to outline the process of building the notebook
          but besides that we have completed our data visualization phase
        - Discussed with her the first resources she should use to get setup and oriented
          with Touch Designer!
        - From this point on she will be helping me produce material for the astronomy
          multimedia performance
      - - Met on friday
        - Discussed her feature piece for the data visualization section of the independent
          study
        - Worked on creating data that would lend itself to a heatmap representation
          of the squirrel population of central park
        - Additional experimentation in the direction of "details on demand" by providing
          interesting select "notes" from the data when a user mouses over a tile
          of the heatmap
        - Preparing for the wrap up week of the web visualization material
      - - Had our usual friday meeting
        - Discussed advanced interaction via geometric/semantic zooms, and data brushing
        - Spent the last 30 minutes discussing her feature piece for the data visualization
          section of the independent study
        - Have settled on doing a visualization of the Squirrel Census data hosted
          on Github
      - - Met up and answered questions related to previous week's exercises
        - discussed intricacies of D3 data binding
        - Covered the dynamics of adding and removing data without a key function
        - Moved on to some very basic code for reacting to user generated events such
          as "Mouse Over" "Mouse Out" and "Click"
      - - Met and took care of questions for the week
        - Moved along to using transitions and animations for our data visualizaions
        - Started looking at how D3 manages changes to the visualization's underlying
          data (enter,update,remove selections)
        - Assigned exercises for the week
        - Mentioned that she should attend the weekend workshop on using p5.js for
          website element creation, and she was one of the 5 people
      - - Met and addressed questions about the first week of using d3
        - Moved away from the template literal svg creation that is in vogue, and
          revisited the roots of d3 and DOM management with selections
        - Invited her to participate in the Argonne GPU hackathon as an extension
          of the datascience side of her independent study.
        - Worked on using the <g> element to organize our data in the graphics, and
          apply transforms to many elements at the same time.
        - Used the <g> elements to learn about how to create axes for our visualizations
      - - Met several times to try to troubleshoot the GLSL shader code on her feature
          p5.js sketch.
        - Shifted into our first week of the data visualization side of the independent
          study.
  - - Stellarscape Astronomy Multimedia Dance Performance
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: true
      status: active
      type: collaboration
      updates:
      - - Spent a significant amount of time trying to learn about Kalman Filters to try to use that for positional estimation based off accelerometer data
        - Ultimately this wouldn't work because Kalman requires a second step where we commpare the estimated position against another sensor's estimation and refine 
        - If we have to bring in another sensor like this we are better off just mounting a second camera above and using that to track 1 dimensional position
        - Also spent most of the past week adding transition behavior to the various effects that I've generated already
        - Preparing for the tech test next week on Tuesday
      - - Had Tech Test meeting last week
        - Switched from developing on the iMac Pro to the performance computer that Kay has
        - Many effects were broken by the switch, so spent most of the week working on what the fixes for these were
        - Read quite a bit of the Kalman filter jupyter notebook "book" by rlabbe, very helpful https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python
        - Going to ask Ryan in TESS if he can install Touch designer on the vis PC connected to the big display wall
      - - Had meeting with Kay and worked on the time codes for the 1a and 2a sections
        - Got positive feedback on many of the changes that I had incorporated since the tech test on the 2nd of September
      - - Worked on feedback provided about movement 1 and 2 effects
      - - Several meetings last week, one with Kay to discuss feedback she had for the visuals I shared with the team in meeting
        - Also met with Gustavo Almeida to discuss use of sensors in the project
        - Learned about the Kalman filter approach for integrating information from the gyroscopes to minimize drift in the IMU sensor output
        - Discussed method for creating sensor recordings so that we don't have to use it live each time I want to develop visuals for movement 2a of the piece
        - Was audience member for Touch Designer E-Sports competition, picked up tons of information about techniques/programs used in the VFX industry from this
      - - Organized all new visuals and research renders into movement folders for meeting that happened on wednesday
        - Had successful meeting and can relax a little bit now
        - Will have meetings with Kay He, Gustavo Almeida, and Win Burleson to work out more revisions to visuals and to chat about sensors
      - - Deadline appears to be less demanding and more work seems to be hitting
          the mark
        - Produced several visual outputs from theoretical astrophysics simulations,
          one of which required the HPC for the visualization using a juypter notebook
          and the Datashader python library
        - The second of which has snapshots that are small enough size of render within
          touch designer still
        - Spent the remainder of the week making modifications to existing effects
          for movements 1 and 2 of the show in accordance with feedback I've received
          from the director
        - Just found out that several internationally known multimedia artists have
          been approached about working on this show also, and one of them seems to
          have agreed which is very exciting.
      - - Director decided that we are going to try to finish all the visuals in the
          next month which meant I had to respond with my own sense that this won't
          be entirely possible
        - Had a meeting to discuss remaining visual to finish and what the priorities
          are
        - Started the process of transferring the Northwestern star formation simulation
          to hard disk to produce visuals from it
        - Checking on the machine that had the Agora Plantary disk simulation on it
          and that Jetstream machine is provin very difficult to access for some reason
        - Managed to work with Rishav Kumar who is Gustavo Almeida's student to send
          data over bluetooth from a wearable device to touch designer
      - - 1 tech test in person session at Crowder Hall
        - Created several new effects and tuned up the transitions between several
          of the fluid dynamics driven particle system
        - Took home a Shimmer IMU to practice wearable position detection inputs for
          touch designer
        - Will be meeting Gustavo's student Rishav next week to learn how to test
          these
      - - 2 tech test in person sessions at Crowder Hall
        - Had the opportunity to see how the material I've produced will actual look
          at the time of the performance
        - Lots of troubleshooting things that happened during the week
        - Recorded footage of Hayley from above for effects development
        - Spent time working on how to perform simple self-advection of velocity fields
        - This will allow a dancer to move their body and have their input spread
          to other particles on the screen they hadn't reached hopefully in a realistic
          way
      - - Had monthly meeting this week
        - presented my work for combining various effects using Hayley's floor dance
          recordings
        - then had development session with Kay on friday trying to get ready for
          the tech test week coming up where we are going to crowder hall in person
          to experiment
      - - Shot footage with Hayley last week
        - had meeting about visualizations that will be used for the floor dance section
          of the performance
        - Got 5 TB of data from northwestern researchers on solar formation
        - Wrote to Evan Schneider about galatic wind data but haven't heard back
      - - Met Kay last week for development session
        - Found that the fluid dynamics effect was broken because a single text file
          was in use by all the different touch networks so development broke the
          stable version
        - Asked researchers at Northwestern for their globus link to the 5TB dataset
          for the star birth
        - Tried compute shader network example for first time
        - Got the perspective, camera raw matrices to work
      - - Several long meetings with Kay working on this project
        - Decided where various Astronomy simulation data is going to fit
        - Made progress on recreating the effect used in the Manna lines video
        - Got 1 snapshot of data from Researchers at Northwestern and will try to
          work this into a touch designer animation
        - Figured out how to parse out the positions from the HDF5 data
        - have to reach out to Evan Schneider about galactic wind time series simulations,
          and start Globus setup for the Northwestern 5 TB dataset
      - - New scientist simulation data
        - Also met with Kay and discussed ongoing work related to lines
        - Set another meeting for this upcoming week
      - - Had monthly meeting last week
        - Discussion about grants that will be pursued by other members on the team
          following our eventual performance
        - Shared my updates about conversation with Gustavo on Sensors
        - Demoed my most recent visualizations using Touch Designer
      - - Wrote code to generate multisegment lines, but got stuck with how to apply
          this with multi line data also
        - Got curious about the scanline fill algorithms used to color the interior
          of geometries that we express with lines
        - Met with Gustavo from the sensor lab to discuss our options.
      - - Worked out how to use the Engine COMP to start processes in other threads
          so that we can keep our visualization performance high
        - This enabled me to implement a TCP client that receives raw bytes and converts
          them to a texture that we can operate on with the GPU
        - I believe this will help me bring the HPC into the project in bigger ways
        - Spent more time learning how to create multi segment lines in the compute
          shader
      - - Switched gears in development for this project to curves instead of particle
          systems
        - Reviewed videos that Kay sent to me for inspiration
        - Spent time reading Fundamentals of Computer Graphics chapters on splines,
          Hermite cubics, and Bezier curves
        - Made a proof of concept GPU compute touch designer example of 100^2 lines
          interpolated to a level of detail around 1000 segments per line
        - Somehow this still ran at 60fps? Not going to argue with the results.
        - Setup meeting with Gustavo the director of the new Sensor Lab to discuss
          the sensor options we will have access to for the show
        - Researched and implemented a system for switching between running Visualizations
          within Touch Designer so that we can script the whole performance from beginning
          to end without being hands on
        - This will be ultimately better because its still pretty likely that we would
          press the wrong thing at the wrong time and create a technical difficulty
      - - Big meeting this week
        - Also heard back from Brant Robertson and Evan Schneider that we can use
          their Galaxy outflow simulation visualization for our show
        - This is such a cool looking effect, I don't know where it will fit in yet
          though
        - Ran into some big performance problems with Touch Designer, but their most
          recent update appears to fix things
        - Made demos for meeting with Hayley and Kay
        - Will do more live outside testing with them next monday
        - Still stuck on creating simple propagation effects
        - Watched new series of videos by Stanslav Glasov, but he didn't go into the
          subject I was looking for on how to generate point/line network effects
          in touch designer on the GPU
      - - Rapid prep for presentation of work with our dancer Hayley Meier
        - Spent the week working on all of our existing visualizations trying to incorporate
          feedback from Kay
      - - Fixed Gasoline errors
        - Ran AGORA Disc example which is Isolated Milky-Way Like Disk Galaxy  using
          pthread over 10 cores (took 2 days to complete)
        - Created 2 visualization workflows for this within Touch Designer, Volumetric
          renderer, Instanced particle system
        - The volumetric renderer was a useful technique to brush up on but suffered
          from 3D texture size limitations in touch designer
        - The instanced particle system will be a much better approach requiring less
          pre-processing (no 3D texture to create, just parsing a binary snapshot
          from gasoline) and doesn't have visual artifacts produced at certain angles
        - visual results https://drive.google.com/file/d/1KFpEVgSEkSh4ZxSClgoLPJEL_Cp3m9ik/view?usp=sharing
          (instanced particle system) , https://drive.google.com/file/d/1DkSOdYlJpbWoZqremOAbLzGY9XB7OwTb/view?usp=sharing
          (volumetric renderer)
      - - Produced first MONOCOLOR inspired geometric scenes
        - Spent time in the week working with derivative community to troubleshoot
          performance of the geometry shader, it appears that the radeon pro vega
          56 card on the imac pro may not be as performant as we hoped
        - developed a visualization of particles driven by a vector field based on
          the mathematical operations curl and divergence and the mouse input
        - Converted fluid mechanics particle system to be driven by kinect depth sensor
          mode
        - Experimented with James Wadsley's SPH program "Gasoline", but cannot get
          past an error which comes up on the Jetstream cloud instance when running
          with pthread on 6 cpu cores
        - So far no suggestions from the developers on the github issue I created
          for this
      - - Produced particle system visualizations driven by fluid mechanics coupled
          to user input (mouse or microsoft kinect)
        - Created first particle system with interconnecting lines using Kay's audio
          input
        - Discussing presentation options with Win Burleson for the top down dancer
          on stage section of movement 1
        - Found second researcher James Wadsley who may be able to provide solar formation
          SPH (smoothed particle hydrodynamics) simulations
      - - Got commercial license for Touch Designer
        - Exploring the differences from the free version
        - Learning to use the geometry shader section of the opengl pipeline to change
          the primitives that I operate on from points to lines
        - Exploring and recreating the techniques used in MONOCOLOR, Latent Space
          and Fulldome show by Marian Essl https://derivative.ca/community-post/monocolor-latent-space-and-fulldome-environment
        - Hitting snags related to renders that are higher resolution than 1280x720
        - Discussing options for movement 1's camera setups with Win Burleson
      - - Met with Kay to do Space Engine Flythroughs with the new version of the
          program, using its built in recorder
        - She thinks that we have all the space footage that we need for the show
          now
        - Met with Lewis Humphrey of Tech Launch Arizona to discuss what license we
          need for the programs we use in the project, he says we are commercial and
          Kay will try to raise money to pay for Touch Designer commercial license
          (600$)
        - Deadline for visuals of the first movement was Friday, but not everything
          was complete
        - Created a Trello to better track my progress and Kay's suggestions
        - New deadline is the 18th to finish part 1, and the first of april to have
          the second movements visuals
      - - Met with Kay to discuss development of particle system behaviors
        - Had a second meeting with her in the week to test out whether my iMac Pro
          desktop is faster than the resources she has, appears a mixed result
      - - Met with Kay, shot footage of various nebulae (horse head, carina, orion,...)
        - Finished storyboard for first movement of performance
        - Win Burleson met with Tech Launch to discuss Licensing, but haven't heard
          the results of that meeting
  - - Virtual Nature
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: true
      status: active
      type: collaboration
      updates:
      - - Officially made contact with Maria Harrington the researcher at UCF who gave the talk at SIGGRAPH on Virtual Nature
        - She and I are meeting next week to review her next grant submission to the games industry giant by the name of "Epic Games"
        - This company sponsored her work previously and she is re-submitting for the next phase of her grant
        - The idea is to become a vis consultant for one of the faculty on campus who can be her on paper collaborator for a Virtual Nature extension to the Southwest
        - Individuals to reach out to at the UA are as follows, Dr. Greg Barron-Gafford, Ellen McMahon, Dr. Scott Saleska, Bryan Black, Aaron Sparks Bugaj
      - - Will be testing Omniverse with the work machine this week to see if it's a viable way to allow the DCC Omani to work with unreal from her laptop
      - - Made video upload for Omani showing how we will take model data in the `.ply` format and make it visible in Touch Designer using a glsl compute shader
      - - Met and discussed material in touch designer related to creating particle paths 
        - This was accomplished using post processing feedback TOPS and also with a "sliding window" of particle position history 
      - - Watched presentation of Birds of a Feather at SIGGRAPH on "Virtual Nature" presented my Maria Garcia at UCF
        - Had the idea of trying to do collaborations with their group but centered on the southwest instead
        - Need to look for faculty who would be willing to participate in this collaboration
      - - Found an incredibly specific SIGGRAPH session of "representing nature" that we are going to go through and study for our uses
        - Will meet next week to go through it
      - - Met with Omani and discussed new idea to use photogrammtery to create the
          point cloud version of her scene
        - This means we need to automate the taking of high resolution photos from
          her unreal engine scene
        - Left that as a task for her to experiment with next week
      - - Created a new video upload explaining the process of sampling input textures
          and writing color to output texture
        - Met with Omani before that and did a recap of the syntax of GLSL
      - - Met with Omani last week and agreed upon meeting on tuesday's at 1:30pm
        - Will ask her for help converting her unreal scenes to bulk exportable models
          that I can employ the meshlab point cloud conversion on
        - Will be uploading compute shader instructional material to youtube as a
          stream every week on mondays
      - - Changing day of the week that I meet with Omani since her schedule is Changing
        - Haven't met up since we first discussed that change
      - - Met with Josh Levine and he helped me perform Poisson Disk Sampling in Meshlab
          to translate a meshed model into a sampled point cloud as one step
        - Using PyMeshlab this will be an automatable process too
      - - Met with Omani and learned to create squares on the screen via the compute
          shader
        - Had several breakthroughs related to the Mesh-> Point Cloud Conversion
        - Wrote a little python program to sample obj image textures at 3D points
          for each geometry face and write that out formatted as .xyz
        - Next need to try this on some of Omani's plant models if she can export
          them from unreal
        - Got thumbs up from Ash about changes to the internship involvement document,
          now need to find out what's needed from me before the 7th of June
      - - Met with Omani and talked more about how to use Touch Designer for compute
          shaders
        - Corresponded with Ash about internship arrangement, haven't gotten all the
          details kinked out but a working document is here https://drive.google.com/file/d/1a4dsJT1ZWRVSsP7CJB7bCF6RqsKClblq/view?usp=sharing
      - - Didn't connect with Omani last week because of a time conflict on her end
          with building maintenance
        - Started working on a Proof of Concept for helping to convert models into
          point clouds
        - Wrote a js triangle sampling program which will be used when loading pixels
          from an image that is used as a mesh face texture. https://trianglesampler.baylyd.repl.co/
        - Had a big meeting with Ash, Victoria, and  Aaron and it sounds like I'll
          be working with Tech Core students this summer on the Biosphere2 collab
      - - Met with Omani, and started in a new direction
        - We will base our work on her photo realistic nature scene
        - Take models from it and convert into the 2 types of assets that I've identified
          in the tree-hugger video (pseudo lidar, dynamic point paths)
        - She asked to specifically learn more about shaders so I've started teaching
          that material using the shadertoy program, I should mention to her that
          touch is free for non commercial and perhaps it will run on her laptop
        - I have to decide what makes sense to do with the results that I already
          have from the Open Root Sim if we go in this direction
        - Ash Black was interested in this project in our meeting, perhaps find a
          way to make this into work that his students can take on
      - - Fixed issues with converting vtu to obj!
        - Apparently issues stemmed from using 0 based counting in the obj face indices
        - Gave Omani  full folder of simulation results to work with
        - Started email chain with Developers at Open Root Sim about how to change
          the simulation parameters to create different plant root systems
        - I believe we won't get very far with this because they only focus on a few
          model plants and nothing bigger than a corn plant
        - Looked into L systems for generating artificial root structures, but it
          would be great to get scans of some much bigger plants
        - Omani started working on the VFX of animating particles traveling along
          the outsides of the mesh, but I think that's going to take time
      - - Still have significant issues converting the .vtu data to .obj
        - Even branched out to looking into the .ply format which I could then skin
          with blender later on
        - Small breakthrough related to discovering Paraview (which opens .vtu without
          issue) can export to gltf, but so far there isn't any clear way to automate
          this process
        - Even the pyparaview doesn't include much information about how to do this.
        - Spent more time looking into using the python VTK package to export
        - Researched writing L-Systems for synthetic branching strutures, and created
          a Rust Package to generate the L-System Axiom, and then wrote a parser that
          places points along the structure
        - Can view the results in Meshlab by importing .xyz formatted point cloud
          https://drive.google.com/file/d/1pJUg8obP_yyV05M8ESPK11xaVZUsyNnI/view?usp=sharing
      - - Met with Omani
        - Started working on the vfx out of unreal engine
        - There doesn't seem to be many easy ways to control the speed at which some
          effects propagate
        - Uploaded Open Root Sim to Jetstream and ran a bunch of sample simulations
          to give us root meshes to work with
        - Spent most of my time re-writing the .vtu to .obj script
        - Facing a really annoying bug in which there's an extra face at the end of
          each triangle strip, but there's no clues as to how to make sure I'm indexing
          the correct number of vertices
        - Once I have this worked out I can send things along to Omani
      - - Met with Omani
        - She was able to present some of her previous examples of biological scenes
          using unreal engine, super impressive
        - Plan is to make use of unreal's effects bound to wireframe models
        - I'll now try to produce a large number of converted meshes that she can
          try to import for running various types of VFX on
      - - Produced multithreading rust program that can do our "point to texture"
          conversion
        - from here I generated a brief example animation to demonstrate how we can
          use these textures with particle system visualizations
        - https://drive.google.com/file/d/13mIKrDNFu6SYvQSZCQxqFJnntc7HCl0y/view?usp=sharing
        - Aaron Bugaj notified us that the green fund is no longer a viable option
          for funding the media vis walls grant
        - Treating this year as a prototype of the program for working with students
          that have Biological System visualizations they want to do
      - - Started working on exploring unreal engine offerings related to our eventual
          visualization
        - Omani says the mist engine might be what we need for particle systems?
        - Created https://repl.it/@baylyd/quadtreeroot#script.js ,https://quadtreeroot.baylyd.repl.co/
        - Started working on Numpy method of calculating Luminosity of texture based
          on the distance that pixel lies from the nearest root point
        - Will use this texture for particle movement against or with value gradient
        - Ran into memory limit for certain sections of the root system on a Jetstream
          allocation
      - - Omani now has DCC status, which is great
        - Met and she got unreal engine installed on the machine that is more stable
          than the laptop
        - She's going to continue to brush up on her python
        - I'm going to try to create the first 3D texture of our first root so that
          we can use other programs than p5.js to do some of our first animated visualizations
      - - Succeeded at parsing the converted .vtu file into a mesh surrounding the
          points of the root simulation
        - Started a small example openprocessing sketch to explain the work to others,
          and myself at a later date
      - - Made more progress on understanding the .vtu file format and its layout
          of data, useful for visualizing the results of Open Root Sim
        - Began work with wireframe visualization of Zea Maize root development
  - - Collaboration With Techcore'S Summer Internship
    - contacts: Ash Black, Victoria Ogino, Aaron Bugaj, Joost(?)
      dates: null
      description: null
      links:
      - https://hackmd.io/xi_m4Kj6QDenBR3ZAfN3iw?edit
      - Project folder on gdrive: https://drive.google.com/drive/folders/13v9QfUFVjQD-x7dh8chQZFvAxmy5zmx6?usp=sharing
      - videos: https://drive.google.com/drive/folders/18tT28oLiXFH1wH8NGUwSU1K0URyzJq9o?usp=sharing
      - Pecha Kucha style presentation: https://docs.google.com/presentation/d/1n6ggKJ-oG7fqHfVsLMnuYkAVh095B1RA7EcNx1RQ12M/edit?usp=sharing 
      - presentation video: https://www.youtube.com/watch?v=EiQ9S5lNbA8
      newUpdates: false
      status: complete
      type: collaboration
      updates:
      - - Just had the final presentation for this project
        - Worked with Trevor to render out the last of the videos that we are using
        - Got stuck trying to use gdrive for streaming our 4k 360 videos, apparently we went over the download cap set by the google drive api
        - Had meeting with researcher Laura Meredith to make sure that our work was in line with her standards for the experiment
        - Worked with Nick Vinas to finish up his animated Vega Lite Graphs
      - - Completed incorporating the xorshift PRNG algorithm into the compute shader
          and it works much better for random uniform 3d motion for gas particles
        - successfully incorporated Laura Merediths CO2 and N2O data into the visualization
          such that the concentrations in the data reflect in the transparency of
          the total number of particles
        - Generated a background population of particles to suggest the existence
          of gas particles not just at the locations where the probes were, but without
          apparent changes in concentration
      - - last week the students had the chance to go to the biosphere 2 rainforest
        - Also we found out that we aren't going to have the new lidar data coming
          from Joost so we are changing to supporting a photogrammetry worflow
        - Trying to keep the interns busy working on things that fit with their respective
          interests, but the creative track students are a little hard to keep busy
      - - Switched gears to using Unity for our output videos
        - Lots of learning that I have to do in order to make this work
        - Figured out how to create custom scriptable render features/passes, compute
          shaders/buffers, material property blocks, indirect GPU instanced Mesh Draw
          calls, and some of their ShaderLab /HLSL code
        - The performance of graphics in Unity is much better than I expected and
          it might be a program I use again after this project
        - The other students are doing well, and have  figured out how to ask eachother
          for help where expertises apply
        - Farabi is doing great work on the Website and the WebVR, Trevor is learning
          all the low level graphics I can throw at him, Samantta is doing great work
          with Adobe Illustrator
        - Nick and Jai both need more direction on what should be done with Python
          and the modeling, but I think that's just until they find their feet
      - - Had first meetings with my interns, Jai Stellmacher, Trevor Hoshiwara, Samantha
          Garcia, Nick Vinas, Al-Farabi Muhtasim
        - Nick and Jai will be the data cleaners and 3D modelers
        - Samantha is going to be the creative/aesthetics lead
        - Trevor will be the glsl touch designer assistant
        - Farabi will be doing the front end development for the final outputs
        - We will be producing 360 video content showing rainforest lidar scans with
          animated research data of NO2 and CO2 recordings via probes at one location
          and different depths over time
      - - Several meetings with Ash and Aaron last week
        - Met researcher Laura Meredith from biosphere2 rainforest who's research
          data we will be using
        - Had meeting for me to lay out the way the output is going to be created
          in the internship
      - - Started work with Trevor
        - update the working document with material related to how I want to approach
          the internship outputs
      - - Met Ash, Aaron, and Trevor Hoshiwara to discuss the first steps of the project
        - Getting a meeting with Joost and Laura to discuss their actual data that
          we will combine with the lidar material
        - Tentative plan is to aim for generating 360 video that can be watched in
          a headset
        - This will allow us the most freedom for the steps involved in combining
          the static structural data with the dynamic scientific data
      - - Went through  my email and found the first Rainforest biome lidar point
          cloud data set
        - Haven't heard from Joost and Aaron about whether I'm allowed to copy the
          file to my own systems to attemp to visualize though
        - Aggreed to have a kick off meeting with Ash and other members of this collaboration
          on June 3rd
  - - Jason Hortin Holographic Dance Graduate Project
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: false
      status: active
      type: consult
      updates:
      - - Helped Jason connect with Bryan and Tech Core with idea that his holographic
          dance project could be part of the Tech Core summer internship
        - Last I heard he had connected with Bryan to do some volumetric video capture
          in the Digital Humanities new center
      - - Wrote another email to Jason and mentioned he should get his project into
          the running for projects listed in the tech core summer Internship
        - He also reached out to Dr. Bryan Carter recently to see what the options
          were for doing volumetric capture footage recording
        - Sounds like more will happen in the fall because he's going back to Chicago
          for the summer
  - - Radiology 1St Year Resident Carl Sabotke
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: false
      status: active
      type: collaboration
      updates:
      - - Sending out introductory emails for Carl to Win Burleson, Ash Black, Bryan
          Carter, and Jenn Nichols
      - - Met Carl for the first time with Chris Reidy and Blake on Thursday
        - Said I would introduce him to all the folks that I know of who might be
          able to help with his AR visualization needs
        - Probably can't do much myself because I'm not familiar at all with the Hardware
          he's trying to use (Hololens)
  - - Remote Visualization Infrastructure Development spring 2021
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: false
      status: complete
      type: infrastructure
      updates:
      - - Spent part of last week learning about "transfer encoding chunked" messages
        - These make it possible for a http client to continuously receive data using
          a ReadStream and may be a method for sending streams of data from HPC when
          on a VPN session.
        - Tried to get started with Omniverse on the Exxact system that I was a test
          driver for, but got stuck waiting for approval to use their docker containers
      - - Just got access to A100, will attempt to use it for some Omniverse testing
      - - Attended the RMACC conference and saw quite a few really interesting presentations
          featuring in-situ visualizations using HPC
        - Notable speakers were Kenny Gruchalla, Steve Cutchin, George Takahashi,
          Philip Black & Patrick O'Toole
        - Corresponding with Gianluca about nvidia related things (Numba graphics,
          and vulkan setup things)
        - Learned the term Pixel Streaming, which means displaying somewhere different
          than the compute
        - Started using Jax as a way to make use of GPU's from python Linear Algebra
          code, could be very useful for some basic headless renderings
      - - Had meeting with our Nvidia reps Bruce McGowan, and Gianluca Castellani
        - Asked them all my vulkan questions, and they reported that we should actually
          see that we have support?
        - Adam helped uncover that we are missing the /etc/vulkan files related to
          the icd.d and implicit-layers.d folder
        - This suggests that the installation on Ocelote may have been unsuccessful
          in some regards
        - Bruce and Gianluca are now helping me try to figure out the issues related
          to Numba and Jax to see if I can run simulations through these
        - They recommended that I try to attend RMACC which is a HPC conference happening
          next week which may have interesting examples of sci vis on the HPC
      - - Met with Chris to discuss vulkan support
        - He's still working on NoMachine configurations,
        - Perhaps going to setup the vulkan 1.1 when the x server is live and see
          if I get different errors
      - - Met with Chris and discussed remote visualization options
        - Sounds like we should look into other tools that support rendering vulkan
          windows that aren't just NoMachine, still need to get to the bottom of that
          conversation with Chris
        - Discussed Omniverse usage
        - Tested Vulkan on each HPC and each have the same Invalid Driver message
        - Created a vulkan issue on their tracker https://vulkan.lunarg.com/issue/view/608706d05df112e5e0a11a5a
      - - Chris notified me that NoMachine was installed on i18n16, but there's firewall
          issues with port 4000 so I wasn't able to test this out
        - Met other members of the Omniverse team who are willing to help me when
          I run into issues with remote visualization for my projects
        - Tangentially related, I figured out how to render raw byte streams of data
          over TCP in the Touch Designer program which may be a personal method for
          doing remote visualization from simulations running on the HPC
      - - Met Peter Messmer of Nvidia's HPC remote visualization division at Nvidia
          GTC
        - Discussed several projects that I want to leverage their tool Omniverse
          for
        - Got positive feedback that streaming results to displays like CATalyst's
          big display walls or even Biosphere2 should be included in their  Omniverse
          Beta release that's out now
        - Met quite a few other individuals who will probably be able to assist with
          this going forward
        - Will be testing using Exosphere and my own local machines before looking
          into HPC -> Catalyst
      - - Created brief statement for Blake to submit to the All hands accomplishments
        - Met with Chris Reidy to discuss the newest developments
        - Gave a demo showing the 1.5 million particle attractor using Jetstream Exosphere
          and the NoMachine remote desktop
        - Started working on how we can use Vulkan on the HPC
        - Chris said he would try to install NoMachine on i18n17 so that we can test
          whether that fixes the `Incompatible Driver` error when running `VulkanInfo`
      - - Big breakthrough learning more webgpu with the Rust systems programming
          language
        - Now understand enough to create some of the simple visualizations I've produced
          in Touch Designer
        - This means a significant improvement in performance because its written
          almost from scratch in gpu modern (vulkan not opengl) graphics code
        - Ran demonstration on 3 different machines of particle system where the mouse
          was an attractor
        - On new desktop was able to run 15 million particles close to 60 fps https://drive.google.com/file/d/1yyVqGcrgrW1rjqE8DPML8LDsJVetj1h1/view?usp=sharing
          Using laptop as nomachine display for exosphere cloud instance with 1/4
          of V100 was able to run 1.5million particles in realtime
        - Performance dropped on exosphere using 15 million particles
        - Video links https://drive.google.com/file/d/1JbMX0GnxqVZgED8vcfoNEKGmEeYaCPld/view?usp=sharing
          (1.5 million), https://drive.google.com/file/d/1zg-fUDU4cYLmW-bksi3GBwplzqWnTBj7/view?usp=sharing
          (15 million)
      - - Finished converting a working graphical simulation of flocking points
        - This is a quintessential n-body update algorithm that would be a good indication
          of the workflows capability for running compute tasks on the gpu and rendering
          the results in realtime
        - Will be testing this out on exosphere this week
        - If that's successful then I will work on getting the example setup on our
          HPC
      - - Succeeded in using Vulkan via NoMachine remote desktop running on subdivided
          V100 Nvidia GPU
        - Created a screen cast video explaining the setup steps and sent to Jeremy
          Fischer in charge of NSF's Exosphere
        - He's very excited and is going to use the video to create documentation
          for users interested in my workflow when Exosphere is released to the public
      - - Worked on Jeremy Fischer's exosphere Nvidia JS Ubuntu 18 instance
        - Worried that their Nvidia V100 gridded VGPU isn't going to actually work
          for Vulkan which will limit the access to modern graphics development that
          researchers can do in their instances
        - Uncertain whether this is going to predict whether Vulkan will work with
          our V100s and K80s since they aren't currently being subdivided into virtual
          GPUs
        - Will be trying all of my steps on Exosphere outside of singularity containers
          this week to determine if singularity is the problem
      - - See whether I can use the singularity containers in the HPC testing environment
          that Chris has setup on Ocelote
        - If that works out, transfer the containers to the exosphere GPU instances
          I have from Jeremy Fischer and test there
      - - Successfully configured new desktop to work as build environment for the
          singularity containers that will have remote visualization/advanced graphics
          capability
        - Did test with nvidia/vulkan (openGL's successor) and was able to get graphical
          windows from the vkcube, and all of the rust gfx-hal demos
      - - Received new desktop machine with Nvidia card inside
        - Started trying to develop containers for use with HPC remote visualization
        - Windows Subsystem for Linux 2 is stuck with certain Nvidia Utils like nvidia-smi
  - - 3D & Vr Retrofit Azlive
    - contacts: null
      dates: null
      description: null
      links:
      - https://rtdatavis.github.io/#retrofitAZLIVE
      newUpdates: false
      status: complete
      type: collaboration
      updates: null
  - - Discussion about Social Network Analysis Visualizations
    - contacts:
      - Leih, Rachel - (rleih) <rleih@arizona.edu>
      - Gildersleeve, Rachel - (ragildersleeve) <ragildersleeve@arizona.edu>
      dates: null
      description: null
      links:
      - NA
      newUpdates: false
      status: complete
      type: consult
      updates:
      - - I received a message from Jean McClelland mentioning there are two faculty
          lookin for assistance with Visualizations for their project that involves
          Social network analysis
        - Set meeting up for this upcoming Tuesday
        - Leih, Rachel - (rleih) <rleih@arizona.edu>
        - Gildersleeve, Rachel - (ragildersleeve) <ragildersleeve@arizona.edu>
  - - Advice For Thesis Defense Visualizations, Sabrina Nardin
    - contacts: null
      dates: null
      description: null
      links:
      - NA
      newUpdates: false
      status: complete
      type: consult
      updates:
      - - Answered follow up questions about her new stream graph over email
      - - Met with Sabrina to discuss her dissertation defense's visualizations
        - Her data is visualizing 8 different violent events from Italy's history
          in the last 50 years covered by 3 different newspapers
        - Most of her questions were about how to improve her existing approaches
          so I laid out the foundation of "task abstraction" taught to me by Joshua
          Levine using Tamara Munzner's design theory for visualization
        - Shared resources with Sabrina and recommended a few changes, but overall
          tried to equip her with the ability to critique her own work
        - Mentioned other best practices such as sharing her visualizations with as
          many other people that are like her target audience as early as possible
        - Has asked for a follow up based on changes incorporated since the meeting
  - - Argonne Gpu Hackathon
    - contacts: null
      dates: null
      description: null
      links:
      - https://drive.google.com/file/d/1yzTyizIMLxRXabMTQgk7KzECsVYHrXOf/view?usp=sharing
      newUpdates: false
      status: Complete
      type: infrastructure
      updates:
      - - Got email that due to limited space our team and its proposed project weren't
          admitted
        - it's ok, I extended a webgpu rust boid simulator example and have it working
          on exosphere so perhaps I got something similar to the experience of the
          hackathon from that
        - https://drive.google.com/file/d/1yzTyizIMLxRXabMTQgk7KzECsVYHrXOf/view?usp=sharing
      - - Should hear from the organizers about whether we are admitted this week
      - - Blake asked if he could join, so I spoke with organizers again
      - - Dima asked if he could join the team so I wrote to the organizers to update
          our roster and they allowed it
      - - Submitted an application to participate in the Argonne GPU hackathon which
          takes place at the end of april
        - Team mates are Sara Willis, Chris Reidy, and Abby Collier
        - Our project is to produce a gpu accelerated flocking simulation that we
          can remotely visualize from a demo created to run only on a single cpu
        - Waiting on updates about our application
  - - Autamus Web Interface
    - contacts: null
      dates: null
      description: null
      links:
      - https://rtdatavis.github.io/#autamus_interface
      newUpdates: false
      status: complete
      type: infrastructure
      updates: null
  - - Bio5 Virtual Reality Tour
    - contacts: null
      dates: null
      description: null
      links:
      - https://rtdatavis.github.io/#bio5-vr-tour
      newUpdates: false
      status: complete
      type: collaboration
      updates: null
  - - Bryan Carter Photogrammetry
    - contacts: null
      dates: null
      description: null
      links:
      - https://sketchfab.com/3d-models/churche-maze-photogrammetry-d0767af08a5d498fb960efe3ac83385f
      - https://drive.google.com/file/d/11E5_912vW6kuPBxcujGKs-UQD9S3jA-H/view?usp=sharing
      newUpdates: false
      status: complete
      type: consult
      updates:
      - - Tayvien Ahmaud William's photogrammetry using Apporto and Metashape completely
          worked!
        - Very impressed with the final result, and I hope I can ask him how he managed
          to work around the timeout limit on Apporto
        - Full version of the model can be found at https://sketchfab.com/3d-models/churche-maze-photogrammetry-d0767af08a5d498fb960efe3ac83385f
      - - Met with Tayvien and tested out whether Metashape is going to be able to
          handle the number of images he has
        - Sounds like there will probably be a need to use HPC just because the runtime
          is a bit too much for apporto?
        - Should reach out to him and ask when his finals are over so that we can
          meet again and get situated on the HPC
      - - Meeting Tayvien who works with Bryan on Thursday this week
        - Will spend some time getting him started with apporto and metashape to test
          things out
      - - Met Bryan last week for a Metashape Demo
        - Talked about how access might work with the HPC
        - Need to explore the setup of Tyson's license inside the HPC environment
        - Discussed using UA's Apporto as a means of getting his studentt up to speed
      - - Tyson and Bryan were able to meet, but I could attend
        - Have a meeting with Bryan set for first week of March to discuss HPC and
          Metashape workflows
      - - Met and discussed array of different upcoming potential partnerships with
          the new center for digital humanities.
        - Potential project to explore using the Nvidia Omniverse for streaming the
          results of HPC calculations/render to center for digital humanities
        - Discussed meeting with Tyson to get started learning to use Metashape for
          his photogrammetry needs
        - Discussed using jetstream allocations instead of their AWS accounts to provide
          on demand access to media for their research projects from user's phones
          (didn't go into details about this)
  - - Covid Retail Mitigation Web Scraping
    - contacts: null
      dates: null
      description: null
      links:
      - https://rtdatavis.github.io/#retailscraping
      newUpdates: false
      status: complete
      type: collaboration
      updates: null
  - - Force Directed Biochem Networks
    - contacts: null
      dates: null
      description: null
      links:
      - https://rtdatavis.github.io/#biochem-networks
      newUpdates: false
      status: complete
      type: collaboration
      updates: null
  - - Judging The Data Visualization Challenge
    - contacts: null
      dates: null
      description: null
      links:
      - https://ual-odis.github.io/data-viz-challenge/
      - https://arizona.figshare.com/search?q=visualization+challenge
      newUpdates: false
      status: complete
      type: community
      updates:
      - - Completed this, my favorite entries also were the ones that won so at least
          I was judging along the lines of the others
      - - Two more entries of 15 to judge, had a few issues with the rubric in the
          process
        - Likely will circle back just to make sure I'm executing my judging process
          consistently on some of the more nuanced scoring categories
      - - Started reviewing the entries, on track to finish this before the 28th
      - - Was notified by Jeff that this is the week to review the metric, and next
          week we will begin reviewing submissions
      - - Planning on approaching a few departments with the offer of a data visualization
          introductory piece for their grad students
        - Will start with my home department of Neuroscience and perhaps Speech Language
          and Hearing where I've done quite a bit of work already
      - - Met with Jeff Oliver and Kiri Carini to host a peer review session
        - Several individuals showed up with questions about whether their projects
          qualify
        - At the end we discussed options for Kiri to join our Data visualization
          roadshow for departments interested in geospatial visualizations
        - Reached out to Stephen Rains in the Department of Communication to offer
          a presentation for his upcoming Computational Social Science (CSS) mini
          conference, but he said the sessions were all full and they didn't need
          more presentations
        - He did send our email offer for intro to data visualization presentations
          to Joe Galaskiewicz who's the director of the CSS certificate program
      - - Spoke with Jeff Oliver about helping with a peer review session for individuals
          participating in the data visualization challenge
      - - Started going over the website for information related to my judging duties
      - - No updates this week
  - - 'Migrant Forensic Empathy Project: A Digital Borderlands Grant Initiative'
    - contacts: null
      dates: null
      description: null
      links:
      - https://mfemigrantdeathmap.baylyd.repl.co/
      - https://devinbayly.github.io/digital_borderlands_conversion/src/mesh_test.html
      - https://devinbayly.github.io/digital_borderlands_conversion/src/index.html
      - https://hackmd.io/Qo_HmwmwSTG-QPYVTJl0Lg?view
      - https://osf.io/v9swc
      newUpdates: false
      status: complete
      type: collaboration
      updates:
      - - Turned in my documentation and project overall to Jason, and will await
          any students he brings forward to pick up where I left off
      - - Spending all my time on this writing the final deliverable wiki and OSF
          project page
      - - Met with Jonathan, and heard that his grants weren't accepted by the library
        - Discussed current state of the project and got his go ahead to write a wiki,
          and then transition the project to closed
        - Will be including sections around what remains to be created and some tips
          about how a student might proceed with such things
        - https://devinbayly.github.io/digital_borderlands_conversion/src/mesh_test.html
        - https://hackmd.io/Qo_HmwmwSTG-QPYVTJl0Lg?view
        - Created OSF project to house additional files and the wiki so that there's
          a DOI associated with the work for when Jonathan publishes about it https://osf.io/v9swc
      - - Sent email to Jonathan to update and request meeting next week
        - Will be ending active development in 2 weeks
        - Created bi directional transport between shrine and the new 3D portal
        - Landscape teleportation via clicks on the mesh
        - Crosses now listen for click events, and create text above featuring whatever
          information we have about the migrant the cross belongs to
        - Crosses now populate/remove incrementally based on the location of the camera
        - Generated UV rotation so that the ground texture doesn't look like 100%
          the same pattern on all parts of the landscape
      - - Lots of time going into this to try to wrap things up by the end of May
        - Unfortunately I've hit a considerable obstacle that the mechanism I was
          counting on doesn't work for constraining the camera motion to the landscape
          mesh
        - I've spent a significant amount of time trying to research alternatives
          and implementing my own solutions and I'm coming up empty
        - I have posted to SO about this and hopefully that will get some attention
      - - Time to really push on this project
        - discovered that the "shader:flat" method can help eliminate artifacts introduced
          from a renderer that's struggling with shadows on a large mesh
        - rewrote the populate and query side of the quad-tree in non recursive algorithms,
          and dealt with bug that was running my machine out of memory in the development
          of this
        - New ideas about how to integrate the shrine into the landscape in a modular
          fashion
        - Will probably make a shallow quadtree for querying the user position every
          5 seconds to see if we need to render more or less crosses in the environment
      - - Got answers about the visual artifacts on SO, sounds like it is a camera
          parameter issue, hopefully I can still resolve it using a shadeless environment
          texture
        - Re-implemented the quad tree algorithm to allow for subsection overlap,
          unfortunately this caused my recursive algorithm to stack overflow when
          I put greater limits on the number of elements per region
        - Re-learned some more advanced methods in rust that will allow me to write
          the "populate quadtree" and "query" algorithms without recursion
        - For the mean time, just started long running task and processed cross placements
          much more accurately
        - It's pretty heartbreaking to see the crosses that are right next to each
          other  https://test-cross-placement.baylyd.repl.co/
      - - Created realistic scaling version, but found that there's visual artifacts
          on the faces far from the camera, created SO issue for this but I imagine
          I will just have to downsize the mesh and alter other aspects to make the
          landscape scale feel appropriate
        - Spent a while re-writing the cross placement Rust code, so that the quad
          tree regions feature overlap
        - This ultimately makes the program less performant but the results are much
          more accurate
        - Performance benchmarking by isolating each individual entry of the scene
          and checking the fps
        - Learned that there doesn't seem to be any one piece causing performance
          issues which is comforting
        - Reached approximately the halfway point of allotted time for this project
          and there's still some big aspects I still have to implement
      - - Big push on this project this week
        - Wrote a Rust program which uses a quad tree structure to organize the 3d
          points of all our landscape vertices
        - When we convert the Medical Examiner's office gps data to mesh coordinates
          we can then query the x,z position of a cross against the quad tree and
          get optimized calculations for the y coordinate where the cross should be
          placed.
        - I then created a demo scene to show the mesh and the crosses placed in the
          landscape https://test-cross-placement.baylyd.repl.co/
        - <10% of crosses placed above mesh in the air, so this next week I'll have
          to do some test cases with a smaller mesh, and known placement points.
      - - Got data on lat-lng for migrant deaths along the border from Jonathan
        - Created simple webmap of this to help understand how many crosses may show
          up in the Aframe scene
        - https://mfemigrantdeathmap.baylyd.repl.co/
        - Unfortunately the previous student has cropped some of the digital elevation
          map (DEM) from its original bounding box so I can't make a function to map
          between the lat-lng range in the data and the x-z coordinates  of the landscape
          model
        - I had to recreate a Marching Cubes workflow to process the a URL to download
          a DEM into a 3D mesh
        - I'll be using the bounding box of this to convert the gps locations of the
          deaths into positions at which I need to place cross models in the landscape
      - - Recreated scene at larger scale
        - Troubleshooting some of the performance issues that are starting to drop
          us to 12fps
        - Experimented with raycasting for interaction with certain objects in the
          scene, performance droped to 5fps from this
        - Took screenshots to send to Jonathan with an update
      - - Heard back from Jonathan that the grants have been submitted
        - He put in a second grant to "recharge" for my time if comes through so we
          will probably have to talk about the finances of that again
      - - Generated first draft of content converted to Aframe https://drive.google.com/file/d/1GAPmOVXS1lgGaoFf_8nSHtGIbJh2XRxk/view?usp=sharing
        - Some issues listed here in my development issue tracker https://github.com/DevinBayly/digital_borderlands_conversion/projects/1
      - - First official week of development for this project.
        - Spent most of the time working on retrieving and modifying the landscape
          model of Southern AZ near "Oregon Pipe National Park"
        - Experimented with the Meshlab python api for programmatically editing/filtering
          large point clouds into meshes
        - Drafted process for systematically importing existing assets into Aframe
      - - Picked up an oculus rift from the library that I can use for 2 weeks
        - I'll try to record some of my previous unity build reviews so that I can
          refer to those as I start the Aframe conversion this way I don't have to
          rent the hardware multiple times
      - - Met with Jonathhan Crisman to get more details about the files that came
          with his folder of student work
        - Looks like i'll need to borrow an oculus in order to demo his scenes so
          that I can see what to create
        - Will reach out to Jen Nichols, and Jonathan  again to get help with this,
          Bryan Carter is where Jonathan got his temporary set so I might start there
          also
      - - Meeting with Jonathan Crisman to discuss beginning questions for converting
          his existing application to the Aframe WebVR platform so that future modification/maintenance
          is easier.
  - - Mt. Lemmon In Your Pocket-Creating A Virtual Reality Tour
    - contacts: null
      dates: null
      description: null
      links:
      - https://rtdatavis.github.io/#GIS_week2020
      newUpdates: false
      status: complete
      type: workshop
      updates: null
  - - Neuro Choropleth
    - contacts: null
      dates: null
      description: null
      links:
      - https://rtdatavis.github.io/#neuro-choro
      newUpdates: false
      status: complete
      type: collaboration
      updates: null
  - - Oyster Vibrio Literature Review
    - contacts: null
      dates: null
      description: null
      links:
      - TBA
      newUpdates: false
      status: complete
      type: collaboration
      updates:
      - - Emily had her defense last week, will now reach out to see if she is happy
          with the project and wants to call it done
      - - Met with Emily and created a leaflet webmap with her lit review GPS data
        - Will meet maximum 2 more times to help her get the website ready for her
          paper
      - - Emily had to reschedule to Tuesday
      - - Sent email to check on status of this as its been 6 months since their last
          update
        - Setup meeting this friday to discuss her needs and advise about data visualization
          techniques
  - - Presentation For Civil Engineering Department
    - contacts: null
      dates: null
      description: null
      links:
      - https://docs.google.com/presentation/d/15Z9zcxU4vIIgFPnKEcaGv9GH7JtjNdx4Xpnjec0EzEc/edit?usp=sharing
      newUpdates: false
      status: complete
      type: workshop
      updates:
      - - Met with Jeff
        - Debriefed Civil Engineering presentation
        - Created action items planning future presentations
        - Discussed Department of Sociology and Anthropology presentation with Kelsey
          Gonzalez for the 16th of April
        - Created planning document with our debrief thoughts, presentation opportunities,
          and content relevant to each individual audience
      - - Met with Jeff Oliver, ran through our presentation to practice
        - Discussed changes, sent the presentation to Joshua Levine to double check
          the attributions for material from his slides
        - Presented to the graduate civil engineers seminar
        - Jeff mentioned that it went so well that he wants to "take it on the road"
      - - Decided on the examples I'll be showing
        - One finite element method visualization of stress forces spreading through
          a model
        - Second one is going to be an Aframe scene of an intersection with car models
          steered by data coming from "CityFlow" the open source traffic simulator
      - - Met with Jeff Oliver last week to build outline for presentation.
        - Got OK from faculty Josh Levine to use material from his slides for the
          "why we do data visualization" and "best practices" portion of the presentation.
  - - Resbaz Organizer And Workshop Provider
    - contacts: null
      dates: null
      description: null
      links:
      - https://hackmd.io/-XS5Mqh8TA2EHjTHCQ_4tw
      newUpdates: false
      status: complete
      type: workshop
      updates:
      - - Presented my Observable notebook workshop last week
        - Had about 10 attendees at the end of the week at 3pm to 5pm which was great
        - Got feedback that I went too fast and that the material wasn't beginner
          friendly so I'll probably re work the presentation notebook somewhat
      - - Met Alex last week and worked on our res baz presentation
        - Extended our notebook so that there's a collaborative live survey built
          into the javascript
        - Fancy way to demonstrate the capabilities of the notebook
      - - Met Chinmay one more time
        - Heard back from Kelsey about what the listing should be for the videos (public
          unless the instructor says otherwise), I think we need to make sure that's
          spelled out for folks attending workshops
        - Got our Observable workshop listed in the webpage
      - - Met Chinmay and we discussed some final todo items
        - Testing our links next week and publishing them to the website team
        - Noticed that my workshop still isn't listed on the resbaz page so I'll have
          to get in touch with Alex about that
        - Blake volunteered to perform the uploads to youtube so that's helpful, still
          haven't heard from Kelsey about whether our unlisted approach is going to
          be the correct way to go
      - - Built workshop Observable notebook
        - Created observable notebook conversion of Kate Isaac's Vega-lite data visualization
          workshop
        - Discussed with Blake what the process would be for adding videos to the
          resbaz youtube channel, it sounds like he'll take that on just because it
          sounds like access is a tricky subject
        - Setup next meeting with Chinmay for following wednesday
      - - Met with Chinmay Joshi
        - Discussed our todo items for getting the zoom links created
        - Discussed scripts to provide the helpers and the workshop presenters so
          that things go smoother than the last time that Chinmay attended
        - Discussed new approach to the recordings, and double checking on Accessibility
          strategies with Kelsey Gonzalez
      - - Met with Alex and planned our workshop
        - Will send outline to Ian Johnson who works at Observable and see what he
          thinks
        - Meeting with other Zoom and Youtube Coordinator Chinmay at somepoint to
          practice zoom and youtube upload
      - - Meeting with Alex Next week
        - More planning conversations on Trello regarding zoom usage
      - - Planning on trello
        - Discussing the needs for Zoom and how to use gather.town for community sessions
      - - Got access settings set correctly for Alex to modify the document
        - Discussing individual tasks with Chinmay for the Zoom and Youtube coordinators
      - - started working with Alex and Kate on a google doc to organize our plans
          for introducing ObservableHQ at the resbaz this year
        - connected with Fernando Rios' buddy Ian Johnson who works at Observable
      - - Met with rest of organizing group
        - Put myself down as one of the two youtube/zoom coordinators since that role
          is similar to how I'm helping the Women's hackathon
        - Pitched an ObservableHQ visualization workshop and Alex Bigelow is going
          to coteach and help with development
      - - Organizer meeting this week to discuss what workshops to offer, and other
          administrivia
  - - Social Vr Museum Capstone Student Project
    - contacts: null
      dates: null
      description: null
      links:
      - https://rtdatavis.github.io/#social-vr-museum
      newUpdates: false
      status: complete
      type: student
      updates: null
  - - Spring Break Covid Photo Maps
    - contacts: null
      dates: null
      description: null
      links:
      - https://rtdatavis.github.io/#spring-break-covid
      newUpdates: false
      status: complete
      type: collaboration
      updates: null
  - - Tech Core Level Up Presentation Monday, Sept 28 2020
    - contacts: null
      dates: null
      description: null
      links:
      - https://rtdatavis.github.io/#techcoresept28
      newUpdates: false
      status: complete
      type: workshop
      updates: null
  - - Tech Core Level Up Presentation Tuesday, Mar 17 2020
    - contacts: null
      dates: null
      description: null
      links:
      - https://rtdatavis.github.io/#techcoremar20
      newUpdates: false
      status: complete
      type: workshop
      updates: null
  - - Virtualgl For Nvidia Accelerated Remote Hpc Visualizations
    - contacts: null
      dates: null
      description: null
      links:
      - https://rtdatavis.github.io/#virtualgl
      newUpdates: false
      status: complete
      type: infrastructure
      updates: null
  - - 'Womens Hackathon: Visualization On The Web Workshop'
    - contacts: null
      dates: null
      description: null
      links:
      - https://womenshackathon.arizona.edu/
      - https://www.youtube.com/channel/UCe1YiJ53o3qcayVs4cipeXA/videos
      - https://www.youtube.com/watch?v=VLwPOtqW8oM
      newUpdates: false
      status: complete
      type: workshop
      updates:
      - - Taught "Making art on the web with code" workshop one more time
        - Low attendance (4 people)
        - Tested using repl.it collaborative coding platform for teaching, proved
          a bit complex and prone to people accidentally deleting the entire program
        - Facilitated other needs with the discord chat such as sharing the youtube
          links when asked for
      - - Provided support for other workshop presenters
        - Assisted with questions related to presenation methods during the virtual
          hackathon
        - Helped upload a few different presenter's videos to the youtube channel
      - - Recorded my workshop on friday evening
        - Second hour on HTML/CSS started recording but didn't finish so will have
          to re-record and upload this week
        - Trouble shot issues with uploads of longer videos on saturday morning
        - Apparently the email notifications for participants didn't go out so I may
          be teaching this again next saturday
        - Had 4 people attend and taught about basic p5.js as well as how to use it
          to create webpage interfaces that can affect the visualizations
      - - Brainstorming and organizing material for my upcoming pre-recorded workshop
          on creative coding with p5.js
      - - Provided feed back to Jen on the website that has the schedule for the event
  - - Xpra And Singularity For Comprehensive Graphical Application Support On Hpc
    - contacts: null
      dates: null
      description: null
      links:
      - https://rtdatavis.github.io/#xprasingularity
      newUpdates: false
      status: complete
      type: infrastructure
      updates: null
  - - Meeting With Tyson Swetnam To Discuss Containers And Remote Visualization
    - contacts: null
      dates: null
      description: null
      links: null
      newUpdates: false
      status: null
      type: null
      updates:
      - - Had recent break through using different technology, need to reach out to
          Tyson to discuss "NoMachine"
      - - Discussed approaches to using containers for remote visualization
        - Shared videos of my developments with him, waiting to hear feedback
        - Discussed option for teaching small special interest group session on container's
          equipped for remote visualization in the upcoming container bootcamp
        - He mentioned an opportunity to try out some proprietary nvidia remote desktop
          software, but we will only have 45 days to use it so I'm waiting for a less
          busy time of the semester to follow up
        - He was interested in the work of Florian Feldhaus and his xpra virtualgl
          EGL backend running out a docker container as a way to remove the X server
          dependency for accelerated remote visualization
