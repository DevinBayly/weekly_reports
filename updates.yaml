attributes:
- type
- status
- description
- updates
- dates
- contacts
- newUpdates
- links
detailed_collection:
- - Template
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: true
    status: active
    type: collaboration
    updates:
    - - None
- - Sensor Lab Demo Package
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: true
    status: active
    type: collaboration
    updates:
    - - I had a meeting with Gustavo Almeida to return materials used for the astro
        dance show and he asked me if I could help a group of his student workers
        with generating demo material for the sensor lab
      - I will be helping the students use and learn the touch designer program and
        the godot game engine to incorporate realtime input from sensors in their
        supply to visual outputs
      - These demos will then help future clients of the sensor lab understand the
        capabilities of each tool
      - this will increase equipment rental for the sensor lab, and save time for
        the clients
    - - None
- - Blender on HPC workflow for GIS
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: true
    status: active
    type: collaboration
    updates:
    - - This project aims to flesh out headless rendering workflows on the HPC using
        the 3d program Blender
      - Finished a proof of concept with a GIS Graduate Assistant Glenn Ingram who
        needed to render high resolution landscape data for infographics but all the
        machines he had access to would run out of memory before he was able to finish
      - Using only 16 cores on Elgato he was able to render his 100% resolution landscape
        in 50 minutes
      - This tool and workflow can be used by many individuals to complete their high
        performance renders on the HPC
      - Will write up documentation for confluence next week
    - - None
- - Erin Choi Brain modeling visualization
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: true
    status: active
    type: consult
    updates:
    - - Jenn Nichols from catalyst sent me an email asking if I could assist with
        this project given my background in visualization and neuroscience
      - Erin is a freshman student who wants to print out 3d models to help explain
        ideas in neuroscience literature about morphological differences in brain
        structures of individuals with PTSD
      - I will meet with Erin every other week on Thursday to assist with her progress
        on this providing resources and advice as needed
      - The outputs of this will in theory be a story for the newletter that we are
        producing quarterly
    - - None
- - Kay He Exoplanets
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: active
    type: collaboration
    updates:
    - - Kay is also looking for assistance with extending the stellarscape project
        to material about exoplanets
      - I'm looking forward to helping with this as long as there is time
    - - None
- - Kay He B2 Lung
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: active
    type: collaboration
    updates:
    - - I have been given approval to start this project with Kay as long as I can
        balance my other duties as well
    - - None
- - Martha Bhattacharya Cytoscape installation
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: active
    type: consult
    updates:
    - - I worked with Martha to create a singularity container that had this program
        installed for using on the HPC
      - I showed her how to get started with the interactive desktops and how to launch
        the program from within the container
      - Unfortunately the program isn't configured to natively take advantage of multiple
        cpu cores so it doesn't scale many of it's operations up to the resources
        available on the HPC
    - - None
- - Marcos Serafim Touch Designer visualization of audio file as video data bending
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: active
    type: collaboration
    updates:
    - - Just met with Marcos and will be trying to help him with touch designer to
        make improvements to a video project that he is sharing at an art conference
      - it is a video composed of material around conveying the distorted nature of
        data related to the HIV epidemic
      - I will be trying to use data bending and data moshing techniques in touch
        designer driven by the audio files that he sends me to render out a modification
        to his video at 4k
      - the presentation is in february on the 18th
    - - None
- - Sarah Stueve d3 brushing
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: true
    status: complete
    type: consult
    updates:
    - - "\e[A\e[B"
    - - Sarah attended my drop ins and asked for assistance with using d3 javascript
        visualization library's brushing for interactive visualizations
      - I provided several examples that covered the basics and a more specific example
        related to the work for her assignment
    - - None
- - Volumetric Capture presentation emergency request CDH
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: complete
    type: consult
    updates:
    - - I assisted Daniel Shevelev in getting material together at the last minute
        for a conference presentation related to the volumetric capture rig at the
        CDH
      - Ultimately we just clipped existing depth camera footage and added audio tracks
        to it using the HPC.
      - In the future we may have a collaboration with this group to help them reduce
        their dependency on the Holokit suite of volcap tools that ran too slow and
        caused some of the panic to get ready for the presentation.
    - - None
- - Shane Thomas using python pandas
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: complete
    type: consult
    updates:
    - - I worked with Shane in drop in hours to help merge several spreadsheets for
        his research into a single one
      - We also needed to filter out certain rows based on student responses on a
        survey and merge the different sized dataframes using values from an ID column
      - Much of this was new to me
    - - None
- - Visit remote visualization support
  - contacts: Hung Tran, Martin Liza
    dates: null
    description: null
    links: null
    newUpdates: true
    status: active
    type: consult
    updates:
    - - Got edit permissions added so that I can update confluence visualization pages
    - - had a meeting with Martin Liza about this but couldn't get it working with
        version 3.2.1 and his mac laptop
      - need to experiment with this on my own
    - - Spent more time working on this and figured out how to redirect between different
        machines
      - Waiting to hear back from the client on this before creating a confluence
        page related to the setup
    - - Last week got this taken care of, made all changes suggested by Eric Brugger
        and then coupled the process to the HPC VPN
      - This makes it possible to sign in to open connect and then communicate directly
        to the allocation that is created automatically according to the HPC profile
      - Not sure how this will work out with different versions of VisIT or when people
        want to use different clusters, but if Puma is the target then we are all
        set
    - - Worked with Eric Brugger and Mark Miller to get setup writing a custom launcher
        for the HPC
      - Got allocation to start from the local client VisIT application
    - - This consultation is aimed to make our HPC systems setup the way other large
        compute centers are for using the Visit visualization tool - I've been working
        with folks from LLNL to learn about how the setup for this kind of thing works
        - It's also possible that the client is going to be content with just instructions
        on how to use the hpc vpn to skip doing multiple jumps for their connection
- - Kirsten Ball ENVS
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: complete
    type: consult
    updates:
    - - Worked with Kirsten's Soil Data to try to produce parallel coordinates plot
        of different microbe community percentages under the different categories
        of Irrigation, and Depth of the sample
- - TURN UP Festival Performance NYU/UA collab
  - contacts: Win Burleson, Kay He
    dates: null
    description: null
    links: null
    newUpdates: false
    status: upcoming
    type: collaboration
    updates:
    - - This project will be a very short term collaboration in which effects being
        generated for the astronomy multi media show are going to be recycled to use
        data send from NYU as part of the visualization
- - Volumetric Capture processing on HPC
  - contacts: Bryan Carter
    dates: null
    description: null
    links: null
    newUpdates: false
    status: upcoming
    type: collaboration
    updates:
    - - None
- - Has Faculty Collaborations With Holodeck
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: upcoming
    type: collaboration
    updates: null
- - Ray Tracing On The Hpc
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: upcoming
    type: collaboration
    updates: null
- - Observablehq Portfolio Of Data Visualization
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: upcoming
    type: community
    updates: null
- - CATalyst data studio vis wall playlist
  - contacts: Jen Nichols
    dates: null
    description: null
    links: null
    newUpdates: false
    status: active
    type: Infrastructure
    updates:
    - - Kiri Carini's assistant sent me a bunch of his material and added it to the
        playlist
      - I included some code that would shuffle the entries before displaying them
      - Figured out that if we load the app in two separate web browsers and display
        one on each side of the big vis wall we get 2x the playlist demos
    - - Made changes to the playlist application so that it would support google drive
        static content
      - Haven't figured out how to make it play videos automatically from google drive,
        but in this case it might be recommended that the video gets uploaded to youtube
        and we just use unlisted links that have autoplay supported
    - - Will be integrating in the links from the data Vis Challenge competition in
        other years to feature on the display
    - - Met with Jen and Jeff to discuss putting the visualizations from previous
        years Visualization challenge finalists on the big display wall using my playlist
        technique
      - now just waiting for the links to be sent by Jeff
    - - Talked with Katie the assistant Manager of Catalyst about whether they need
        material for presenting on the screen for the Home Coming weekend, and she
        didn't reply before writing this reply
      - Figured out how to use youtube links in the screensaver web app so now Jen
        Nichols and other individuals can provide video links that they want the bi
        display wall to show on a playlist
    - - Met with Jen and created a working prototype web app that pulls link urls
        from a google sheet to view for a predetermined amount of time before switching
        to the next one
      - This will allow the CATalyst big display wall to remain active in between
        booked sessions
      - Unfortunately they are now having technical difficulties with the right side
        of the vis computer's display and the keyboard and mouse seem to not be interfacing
        correctly with the computer being displayed unless I use my own device
    - - Setup meeting with Jen for next week
      - Goal of short project is to create a web page that will be screen saver for
        the big vis wall at CATalyst
      - Have process that iterates over videos and live demos of visualizations and
        creative code works
      - Make it something others can add to over time
      - Should be a great way to be in the driver's seat for what material gets viewed
        by people visiting the space
      - This will make it easy to have a showcase option for RT's work
- - Thermal Imaging Project
  - contacts: Ed Wellman, Brad Ross
    dates: null
    description: null
    links: null
    newUpdates: true
    status: active
    type: collaboration
    updates:
    - - Created repo home page with full documentation of code, process, screenshots
        and video to help understand what I'm delivering to them
      - Met with Ed to triage some things that weren't clear in the material provided
    - - have been running the detection script on the HPC overnight
      - also made changes to the log plotter so that it interactively draws points
        on a background image at the location that motion was detected ain the time
        span that the user has selected with the cursor brush
    - - Met with team on tuesday and discussed needs in the final part of this project
      - Actively trying to setup the code on a raspberry pi to test whether these
        can be used out in the field at the mines with the cameras
      - Started running code on HPC and devoped a small visualization tool to display
        tick mark results on an axis who's length is determined by the length of a
        video
      - this should help the clients go through 12 hour videos and pick out what sections
        have most activity to watch closely
      - meeting with Chad on friday to discuss the extension proposal
    - - Attended and assisted with the Lowell Geotechnical Center of Excellence shareholder
        presentation last week
      - Provided my diagrams and screencasts for the presentation and got feedback
        that attendees really enjoyed seeing this stuff in action
      - Decided to test out some of the code on a raspberry pi since the groups goal
        is to have these algorithms running onsite at the pit mines
    - - figured out how to have certain shapes and metrics appear on the screen while
        the videos get processed so that I can screen record - started adapting sketches
        to the formal vector graphic diagram
    - - Met and discussed questions related to how to get videos that I can use for
        my testing passes - I also started making python class diagrams for simplifying
        the process of exploring combinations of algorithms in the overall frame processing
        - I shared a doodle of an overall diagram representing the different steps
        of the process with Ed and he's in agreement that this is what they need for
        their December review meeting with NIOSH
    - - Started working on the research for a method of keeping in memory some number
        of detected objects in motion with a way to compare a new frame's detections
        against this list to update existing ones and add brand new tracked objects
        to the list
      - Sent updates email to Brad and Ed earlier this week asking about how the "robustness"
        of the computer vision is going to be calculated
      - Sounds like this is another open question related to the project
      - Looking up implementations of Boundary Box Overlap and the Hungarian algorithm
        for tracking multiple objects
    - - Met with Ed in person and discussed details of the project as well as a roadmap
        and ways to provide updates as the collaboration gets underway
      - I have agreed to work 10 hours a week on this project for October and in November
        I'll have a chance to re evaluate the amount of availability I have to increase
        that amount
      - They have a milestone document dealine that they need my work to be performed
        for in January so that is the next nearest deadline for this project
      - The plan is to begin going through the excel spreadsheet logs to find good
        candidate video phenomena to practice various computer vision algorithms on
        with python's Open CV package
    - - Had first meeting with Brad Ross and Ed Wellman
      - Received their project directory to explore and several documents detailing
        the previous steps of the project
      - Sounds like they want someone to explore a few different types of motion detection
        algorithms by January so they can put that information into next milestone
        document
      - If possible would like to make the workflow run on the HPC so that they can
        process videos in bulk
      - Set monday as date to meet and go over material with Ed
- - IEEE Satellite Vis Conference Planning
  - contacts: null
    dates: null
    description: null
    links:
    - https://www.appsheet.com/start/7e25f1d2-e9a4-4eec-a31a-5edd275f3059
    newUpdates: false
    status: complete
    type: collaboration
    updates:
    - - This was completed recently and the ending of the conference was earlier than
        expected because of a + covid case from one of the participants who had been
        around on the first day of the conference - we followed all recommended steps
        and there were no other positives which is great - Presentation of my lightning
        talk went well, and so did the other events that I oversaw for the conference
        such as the Visualization showcase lunch and the Birds of a Feather lunch
    - - Talked with Barry and Todd to setup a filming opportunity this week of the
        HPC machines for the promo video
      - Sent out more Showcase invitiations to members of the community
      - Helped produce material for the health sciences newsletter advertising the
        satellite conference
    - - This week we planned out the room assignments for various simulcast sessions
        from the main IEEE conference
      - I also booked the catalyst data studio to spend a half day filming for the
        promotional video
      - Josh and Nirav wanted me to also be part of the interview team so I got to
        be on camera talking about Research Technologies and the goal of supporting
        the data visualization community here on campus
      - I also reached out to a few other people who may be interested in being exhibitors
        at the Visualization Showcase Lunch on Tuesday the 26th
    - - Created self updating google form for the Birds of a Feather submission
      - Organized connection for producing the Satellite conference promotional video
      - Attended meeting with videographer and Josh
      - Started thinking about what live streams to watch during the satellite event
      - Reached out to folks who might be interested in showing their services at
        the Showcase Lunch on Tuesday
    - - Had meeting on Friday planning video shoutout material
      - Decided to start emailing folks in the stakeholder list I created about the
        conference that's coming up
      - Rooms and Birds of a Feather timing worked out
      - Had meeting with DSRT folks recently and discussed the conference, folks on
        panel are going to help advertise
    - - Spent 2 hours in meeting working out details related to the location reservations
        for the conference as well as community planning steps
      - Volunteered to create the Birds of a Feather (BOF)  sign up sheet with Google's
        AppSheet https://www.appsheet.com/start/7e25f1d2-e9a4-4eec-a31a-5edd275f3059
      - Also volunteered to be the main person at the information table during the
        conference helping folks identify which vis experts their questions are best
        suited for
- - Vulkan on HPC
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: true
    status: active
    type: infrastructure
    updates:
    - - Still troubleshooting the issues with Darwin in delaware, GLIBCXX version
        appears to be too old to work by default, trying singularity work around
    - - Got a free moment to do a test of creating a singularity container on the
        work machine created with Alec's SHPC program that I then modified with a
        vulkan installation and the rust webgpu code
      - Then transfered this to the HPC and it was able to render graphical windows
        there
      - This frees us up from only using one vulkan version that's installed on the
        HPC, and may extend to other supercomputers as well
      - Issue remains about how to create necessary ICD files when they aren't provided
        on the system by the Nvidia driver installation, will ask Adam about this
        some day
    - - Tested out using OOD with the boids flocking and it works
      - Looked into running rust nannou creative coding library and that still throws
        the "adapter not found" error
      - Isoloated the glib version mismatch to the `shaderc` library dependency, will
        investigate `Naga` or `glsllang` to get around this
      - Next step is to take the locally running HDF5 renderer and test it on ocelote
    - - In the process of creating a program that will read from hdf5 files and transparently
        draw points to image texture using GPU to be saved out as individual frames
        of Astronomy Simulation videos
    - - Reading section on windowless rendering from https://sotrh.github.io/learn-wgpu/showcase/windowless/
      - Hopefully this will make it possible to run simulations steps faster than
        the x11 can forward and then stitch the png results together in a movie file
    - - Got help from Adam and Ric last week and was able to have the wgpu boids simulation
        running on the HPC sending frames to laptop with x11 forwarding
      - Adam also put the vulkan sdk into a module so that we could use it outside
        of r5u09n1 the puma node that he had set aside for testing
      - Started running into new errors `No valid $DISPLAY found. Unable to load module.`
        when trying to do other work on the HPC now though
    - - Vulkan is the successor to OpenGL and much of my low level graphics understanding
        comes from web-gpu that leverages Vulkan
      - This means in order to provide greater visualization support with the HPC
        I need to have access to Vulkan
      - The issues are that our nvidia driver setup has a bug in it related to Vulkan
        support
      - I tried to get around this by configuring a singularity container with the
        files we know to be missing but a new error arises 600 lines later in the
        `vulkaninfo` command now
      - Reached out to UCSD and Expanse HPC visualization personnel to inquire about
        their support for this modern graphics library
      - Sent email to the Campus Champions list serve about this but didn't get many
        useful replies
- - Streaming Technology for HPC
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: active
    type: infrastructure
    updates:
    - - Had a meeting with Ryan and Dave from TESS and they installed cisco vpn on
        the machine that is plugged into the big display wall - Unfortunately the
        original machine that I was going to use there had a catastrophic bios failure
        which is not so good - In the future the options for using the wall as an
        HPC display are now better supported
    - - Had a meeting with Ryan Tombleson from the Main Library TESS IT support group
      - Discussed the workflows that I use for remote visualization that all rely
        on the HPC VPN
      - Asked whether there would be issues with connecting the catalyst vis pc to
        the VPN when we want to do some realtime HPC visualizations either with OOD
        or some customized GUI
      - He's very open to the idea and couldn't imagine any reason why we wouldn't
        be able to do this
      - Set another meeting for the first week of November to test out things after
        Cisco Anyconnect install
      - Then spent the rest of the meeting demonstrating Omniverse and it's Paraview
        connector with the idea that we may be able to use the vis pc omniverse client
        to display the results of operations happening on the HPC if I can get it
        setup with Omniverse singularity
    - - Spent time working out how to use FFMPEG to send frames from laptop webcam
        over udp as concatenated JPEG to an intermediary server that received upd
        and converted that to http Transfer Encoded Chunks received in a web browser
        and displayed as an updating image
      - This will eventually develop into a program that can take researcher jpg udp
        frames and display them where ever the researcher wants to display them as
        long as the display client is also on the HPC VPN
      - This is a simpler remote visualization method than use desktops or VNC because
        it only requires running a script on the HPC, but does mean a researcher has
        to write code to send the images over udp
      - Either way, it should be less work for the Infrastructure team and is a simpler
        remote client setup which just has to be a browser page
- - Data Visualization Roadshow With Jeff Oliver and Kiri Carini
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: true
    status: active
    type: collaboration
    updates:
    - - Signed up for two presentations at this point in the semester, one next week
        the other in april
    - - will be meeting online for the beginning of the semester
      - have our first presentation in the art department earlier in the spring
    - - gave our last presentation this week, only one attendee so I think we could
        be doing something different to increase our numbers
      - Did a full debrief of the semester and figured out some plans for what we
        are thinking of doing next week
    - - Met with Kiri's student to experiment setting up the AR Sandbox which we are
        thinking of displaying at the GIS day on the 17th of Nov
      - Also started planning presentation for the Eller Tech Core Level up students
    - - Had 2 presentations this week
      - RTG presentation for Laura Miller and Kevin Lin which went well
      - Gave presentation for 50 people in the Special Libraries Association, Science
        Technology Community
      - This presentation was more meta because we presented on the way that we give
        presentations to groups around the University of Arizona
    - - Started draft presentation for Math RTG group presentation, outlined sections
        to change for the presentation since the last one we offered
      - Created some faceting examples using the Iris Dataset for the presentation
        and the live demo
      - Incorporated survey link and resources web page that Kiri Carini put together
      - Enrolled for the Visualizing the Future public symposium in which individuals
        at other universities discuss strategies for successful visualization service
        and education in the university context
    - - Met last week and discussed upcoming presentations
      - First is Research Training Group for Mathematics, we will be using Jellyfish
        swimming fluid dynamics data, the big task is simplifying some of this to
        be used
      - Next is the Special Library Association presentation, we will be explaining
        to other librarians our approach to explaining data visualization for different
        groups around campus
    - - Met and revised presentation with Jeff Oliver and Kiri
      - Worked on tool for Kirsten Ball as individual consultation
    - - Setup two new presentations for the fall so the remaining total is 5
      - First new presentation is for Special Libraries Association (SLA) and their
        Science Technology Community sub group
      - Second is the Math department Research Training Group (RTG) with Laura Miller
        and Kevin Lin
      - Had a debrief session from last presentation and we are  going to do a bottom
        to top overhaul of the presentation for the ENVS presentation on the 27th
    - - Had first drop in last week, spent time with Kiri figuring out how the semester
        might look
      - Planned out some presentations at the GIS day in November also
      - Aiming to create demonstration of streaming point cloud gdal stuff from HPC
        the same way the USGS uses Entwine for it's national Geospatial data
    - - Created new flyer for the catalyst studio presentation on the 2nd of Sept
      - Kiri volunteered to reserve our space at the studios
      - Worked on material that needs to be swapped out from the presentation template
      - Prepared material for first drop in time next Tuesday
    - - Planning the catalyst studios presentation that is our first of 6 "Intro to
        Data Vis" presentations for this fall
      - The following presentations will happen and others may be added (ENVS, Math,
        Level up Eller, 2nd Catalyst presentation, Public Health)
    - - Reviewed form for collaborative drop ins with myself Jeff Oliver and Kiri
        Carini at Catalyst
      - Received ENVS seminar data from Kirsten Ball
      - Set presentation date for ENVS on sept 27th at 3pm
    - - Met with Jeff and planned the changes we have to make to the presentation
        for next thursday
      - I decided on the dataset that I'll be using in my part of the presentation
        (SIR model for virus transmission)
      - Sent messages out to Catalyst to figure out a meeting time for presentations
        there in the fall
      - Jeff invited me to attend the Software carpentry instructor training happening
        this fall and I think that sounds like a great idea
    - - Set Thursday next week for  meeting to prepare for first presentation on the
        15th for Laura Miller's Research Training Group in the Math Department
    - - Reached out to Catalyst studios for scheduling our kickoff and wrapup data
        visualization presentations for general audiences
      - Jeff has been in contact with the college specific librarians at Eller and
        Public Health setting up presentations
      - We've already scheduled 2 presentations with another 6 in the works, and then
        I think we will only add more upon request
    - - Flurry of emails week
      - created google sheet to track who's emailing who and why and what the contact's
        affiliation is
      - Sent out close to 10 separate emails which each made several more contacts
      - Set the constraint on performing a minimum of 7 presentations in the fall
    - - Finished writing stock emails for sending out to faculty we believe are teaching
        graduate seminars, and the coordinators who can tell us who else is teaching
        seminars
    - - Met Jeff and discussed approaches for presentations next fall
      - Started creating stock email offer for presentation to send to assembled list
        of contacts
      - Will perhaps target graduate seminars and section leader/TA groups which have
        more availability than an undergraduate class
    - - Setup meeting with Jeff next week to plan our approach to graduate intro to
        vis presentations for the fall
    - - Heard back from the NSCS department about a presentation
      - Just working on the date for the presentation
      - Presenting to undergraduates also
    - - Reaching out to the NSCS department to ask whether they have a graduate seminar
        which we can present to?
    - - Sent offer for presentation to Dianne Patterson in SLHS
      - She isn't able to offer actual class time for a presentation
      - Will have to decide with Jeff if we really don't want to pre-record something
        for them
      - Trying the NSCS department next
- - Independent Study Abby Collier
  - contacts: null
    dates: null
    description: null
    links:
    - https://openprocessing.org/user/255658?view=sketches
    - https://observablehq.com/@aecollier/sqrrules?ui=classic
    - https://aecollier.github.io/portfolio/
    newUpdates: true
    status: active
    type: student
    updates:
    - - Met with Abby, worked on more of the touch designer workshop material for
        the school of art, and started drafting demo ideas for the sensor lab
    - - met with Abby and she agreed to help with other touch designer projects that
        are on my plate
      - she will also probably get connected with NYU and help with the turn up planning
        and visualization work
      - Met today and started working on new material surrounding generative visuals
    - - Met with Abby last week and taught her about the touch designer timer and
        the timeline that things can be locked to
      - This is incredibly useful and makes it so that we can reliably hit certain
        timestamps in the music with visual transitions and events
      - We spent the rest of the time trying to debug some of the effects that were
        having issues
      - Turns out there was a dependency cycle being setup in one of the effects and
        this caused a result that looked like the effect would freeze from one level
        to another
    - - Met up over zoom this last week and worked on adding trails to the fluid driven
        particle effects
      - We also spent time trying to work out the dynamics of the climax effect for
        movement 2a
    - - Started a new github repo for tracking the development and exercises related
        to using touch designer for the astro dance project
      - Worked on parabolic path effects with the goal of generating a visual effect
        for a musical climax cue in movement 2a
    - - Met with Abby and discussed show running techniques
      - This involves taking a step back and figuring out how we are planning to trigger
        start, end, transition behaviors in the effects that we've generated so far
      - Much more python code getting used at this point than before
      - Getting her support deciding how to make effects transition between their
        initialization and running states
    - - Met with Abby and talked about Github actions for automated cloud operations
      - Switched to working on stream disruption effects
      - Discussed a little bit of the Kalman filter bayesian questions that I have
    - - Met with Abby and worked on the 3rd effect from the performance's movement
        1a
      - Left it on a note where I described 5 modifications of varying difficulty
        that she could investigate
    - - Met to work on combining effects for the 1a movement's motion aura effect
      - This included teaching her about glsl atomic counters
      - implemented system by which she can work on the same Touch Designer files
        as me and labeled various parts of Kay's feedback by difficulty level to make
        it easier for her to assist me
    - - Met briefly last week to cover recent updates
      - Discussed plan for distributing the work related to the feedback given from
        Kay in the last meeting
    - - Met with Abby and discussed how to use the multithreading concept of "atomic"
        counters for use in compute shaders
      - worked on several effects where the particles that are generated come off
        of the dancer and move in the direction that the dancer was proceeding when
        the particles were born.
    - - Met with Abby and worked on compute shader stuff for self advection of color
      - we also worked on noise generation inputs for the astro dance performance
        movement 2c
    - - Met with Abby and worked on teaching her the fundamentals of rendering lines
        from glsl custom materials and line primitives
      - Gave her some instructions about updating the example touch designer network
        that we are working with
    - - Met with Abby and worked on numpy boolean indexing techniques to apply to
        masked grid effects for the astro dance project visuals
      - Spent the rest of the time revisiting various ideas of how to create touch
        designer networks for visualizations
    - - Met and setup access to a small jetstream machine for learning more about
        docker and containerization
      - Spent more time attempting a vectorization of lines following all-to-all connection
        pattern in a circle, but got stuck on numpy errors
      - switched to just doing this in touch designer within a single glsl top
      - ended meeting with idea that we will just  be making a grid and then filtering
        based on a radius of a given size to control what points are shown
      - really need to get back to the TCP stuff for transfering data to active touch
        sessions
    - - Met and discussed creating regularly spaced data in numpy using vectorize
        that can be the backbone of touch designer visualizations
    - - Continuing this connection as part of working on the Astro Dance project
      - Met up and discussed using docker containers
      - Shared "Play with Docker" tool to practice docker
      - Showed her how to use ssh to connect with PuTTY
      - Sent data from remote machine over ssh to Touch Designer using tcp
      - Need to refocus on how these techniques are going to help us going forward
    - - Met up and started covering TCP protocol for transmitting images from JAX/numpy
        server into touch designer
      - This helps us extend to creating assets outside of the touch designer system,
        and offload larger tasks to more performant machines
      - Covered some more information about drawing grids with compute shaders, and
        the numpy Script TOP
    - - Spent a good amount of time at the begininning looking through materials provided
        by Kay He to see which effect she would like to work on next
      - Settled on her helping out with the monocolor circular lines
      - Met and started working on compute shader code involving grids
      - Sent off Abby's portfolio to Holly Brown https://aecollier.github.io/portfolio/
    - - Met and added final touches to her feature piece for the final part of the
        independent study
      - Submitted grade with Blake, and will send Holly Brown a brief report with
        Abby's independent study webpage
    - - Abby won the SBS senior award !!
      - Met and discussed how to use actively moving particles positions for sampling
        other textures
      - This is the foundation of reactive particle systems, so we now can use video
        input of a person to manipulate our particle system!
      - Will work with Blake to submit grade before the end of the semester
    - - Had our usual friday meeting
      - Made quite a bit of progress on her feature Touch Designer piece called "purple
        stars"
      - Spent time working on her github webpage to present the work from this independent
        study
      - Talked about potential for her to continue in this capacity after graduation
        as a DCC
    - - Met up and started working on custom materials
      - Had long discussion in the meeting cementing  the mental model of how the
        shaders are using different color channels to move particles
      - Also started working on how to use other texture inputs to help with particle
        spreading patterns
      - Had to rush into using uniform sampler2D types
      - As a result I made a short video recording explaining how to do modular arithmetic
        to calculate texture sample coordinates from instance id's  for each of our
        particles
    - - Met with Abby several times this week
      - The main focus of our time is on how to get her help with the touch designer
        visualizations for the show
      - Kay has requested a recreation of this effect https://www.youtube.com/watch?v=r9dd6csVZbk
        and I'm making it Abby's focus
      - Discussed feedback loops, texture lookups, neighbor queries, and soon we will
        talk about writing custom materials with shader code
    - - Met with Abby and started working on Touch designer
      - Wrote a section of Abby's outstanding senior nomination for Rich Thompson
        who is the primary Python lecturer for ISTA
      - Worked live through a Touch Designer network showing basic features such as
        CHOPs TOPs, parameter referencing, and feedback
      - Then did a very quick intro to programming particle systems as that will be
        the main thing we generate for the astro dance performance
    - - Met on friday to discuss final tweaks to the data visualization feature piece
      - Abby built a very nice heatmap displaying squirrel counts from the data set
        against the hectares they live in within central park
      - Worked on making legend for her graph, and how to add observational notes
        to the data
      - She will add some prose to outline the process of building the notebook but
        besides that we have completed our data visualization phase
      - Discussed with her the first resources she should use to get setup and oriented
        with Touch Designer!
      - From this point on she will be helping me produce material for the astronomy
        multimedia performance
    - - Met on friday
      - Discussed her feature piece for the data visualization section of the independent
        study
      - Worked on creating data that would lend itself to a heatmap representation
        of the squirrel population of central park
      - Additional experimentation in the direction of "details on demand" by providing
        interesting select "notes" from the data when a user mouses over a tile of
        the heatmap
      - Preparing for the wrap up week of the web visualization material
    - - Had our usual friday meeting
      - Discussed advanced interaction via geometric/semantic zooms, and data brushing
      - Spent the last 30 minutes discussing her feature piece for the data visualization
        section of the independent study
      - Have settled on doing a visualization of the Squirrel Census data hosted on
        Github
    - - Met up and answered questions related to previous week's exercises
      - discussed intricacies of D3 data binding
      - Covered the dynamics of adding and removing data without a key function
      - Moved on to some very basic code for reacting to user generated events such
        as "Mouse Over" "Mouse Out" and "Click"
    - - Met and took care of questions for the week
      - Moved along to using transitions and animations for our data visualizaions
      - Started looking at how D3 manages changes to the visualization's underlying
        data (enter,update,remove selections)
      - Assigned exercises for the week
      - Mentioned that she should attend the weekend workshop on using p5.js for website
        element creation, and she was one of the 5 people
    - - Met and addressed questions about the first week of using d3
      - Moved away from the template literal svg creation that is in vogue, and revisited
        the roots of d3 and DOM management with selections
      - Invited her to participate in the Argonne GPU hackathon as an extension of
        the datascience side of her independent study.
      - Worked on using the <g> element to organize our data in the graphics, and
        apply transforms to many elements at the same time.
      - Used the <g> elements to learn about how to create axes for our visualizations
    - - Met several times to try to troubleshoot the GLSL shader code on her feature
        p5.js sketch.
      - Shifted into our first week of the data visualization side of the independent
        study.
- - Stellarscape Astronomy Multimedia Dance Performance
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: true
    status: active
    type: collaboration
    updates:
    - - This was the big re-render week
      - Spent a lot of time with Kay's machine trying to fix issues related to the
        various effects being not quite right for editing into the sxsw footage
      - Just had meeting about how the sensor installation will work when we get to
        austin
      - Have to work on features and testing with custom network
    - - In last couple weeks have been having lots of rehearsals
      - Show is officially postponed but will be having a private premier and the
        next day will be filming the footage for the SXSW trip
    - - Found out the piece will be performed in a few places after the U of A, SXSW
        and Athens Greece
      - Met with the director to make changes to effects in 1a related to getting
        the size of Hayley consistent and correct for each of the effects
      - Figured out how to use an audio device in to control the audio reactive particles
      - Will be using this on friday when meeting with the Taiko Drummer performer
    - - Had last official tech test in Crowder hall
      - Things went incredibly well, we were able to actually put sensors on Hayley
        and step through the full timeline of part 2a of the performance
      - I also found out that we have several other presentation opportunities in 2022: The
          Athens conference on performance and visual art (june 2022), SXSW 2022
    - - Had a tech test rescheduled into Kay's office, and tried to hookup two different
        sensors to our dancer Hayley
      - for some reason only one of these things wanted to work at a time
    - - Went to Kay's office and worked on her show machine
      - Built new system for supporting complex effect sequencing with support for
        restarts, and effect indexing for testing out the effects we have built for
        movement 2a
    - - Spent most of my time this week on research data renders for the show
      - Figured out a new way to load way more data into touch designer
      - This allowed me to enter an entire SPH simulation into memory and render at
        60 fps the behavior or the 100000 gas particles over the course of their 1000
        time step simulations
      - I was then able to write an interpolator so that we could play at what ever
        speed looks best for the fixed media video
      - Tried to render these videos on Kay's machine and the results really don't
        look good for some reason, have more meetings planned to finish this before
        the end of next week
      - Made plans to meet next week to plan out the next tech test rehearsal
    - - Had a tech test this week
      - Spent a long time working out how to stream video frames from a raspberry
        pi connected to a kinect dk infrared camera to the computer running touch
        designer for the purposes of motion tracking, but wasn't able to test this
        is the time we had available
      - Worked out timecodes for all the effects and transitions for movement 2a
      - Agreed to  deliver all requested changes to the video effects between now
        and the 16th of November, following that there will only be changes made to
        the show running program
      - Reached out to Mike to see how the simulation video is coming after we asked
        for it to be extended
      - Connected with Dylan Murphy of the Ista department to ask specific questions
        about the Kalman filter and whether it's possible to use only one sensor to
        implement position estimation
    - - Spent a significant amount of time trying to learn about Kalman Filters to
        try to use that for positional estimation based off accelerometer data
      - Ultimately this wouldn't work because Kalman requires a second step where
        we commpare the estimated position against another sensor's estimation and
        refine
      - If we have to bring in another sensor like this we are better off just mounting
        a second camera above and using that to track 1 dimensional position
      - Also spent most of the past week adding transition behavior to the various
        effects that I've generated already
      - Preparing for the tech test next week on Tuesday
    - - Had Tech Test meeting last week
      - Switched from developing on the iMac Pro to the performance computer that
        Kay has
      - Many effects were broken by the switch, so spent most of the week working
        on what the fixes for these were
      - Read quite a bit of the Kalman filter jupyter notebook "book" by rlabbe, very
        helpful https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python
      - Going to ask Ryan in TESS if he can install Touch designer on the vis PC connected
        to the big display wall
    - - Had meeting with Kay and worked on the time codes for the 1a and 2a sections
      - Got positive feedback on many of the changes that I had incorporated since
        the tech test on the 2nd of September
    - - Worked on feedback provided about movement 1 and 2 effects
    - - Several meetings last week, one with Kay to discuss feedback she had for the
        visuals I shared with the team in meeting
      - Also met with Gustavo Almeida to discuss use of sensors in the project
      - Learned about the Kalman filter approach for integrating information from
        the gyroscopes to minimize drift in the IMU sensor output
      - Discussed method for creating sensor recordings so that we don't have to use
        it live each time I want to develop visuals for movement 2a of the piece
      - Was audience member for Touch Designer E-Sports competition, picked up tons
        of information about techniques/programs used in the VFX industry from this
    - - Organized all new visuals and research renders into movement folders for meeting
        that happened on wednesday
      - Had successful meeting and can relax a little bit now
      - Will have meetings with Kay He, Gustavo Almeida, and Win Burleson to work
        out more revisions to visuals and to chat about sensors
    - - Deadline appears to be less demanding and more work seems to be hitting the
        mark
      - Produced several visual outputs from theoretical astrophysics simulations,
        one of which required the HPC for the visualization using a juypter notebook
        and the Datashader python library
      - The second of which has snapshots that are small enough size of render within
        touch designer still
      - Spent the remainder of the week making modifications to existing effects for
        movements 1 and 2 of the show in accordance with feedback I've received from
        the director
      - Just found out that several internationally known multimedia artists have
        been approached about working on this show also, and one of them seems to
        have agreed which is very exciting.
    - - Director decided that we are going to try to finish all the visuals in the
        next month which meant I had to respond with my own sense that this won't
        be entirely possible
      - Had a meeting to discuss remaining visual to finish and what the priorities
        are
      - Started the process of transferring the Northwestern star formation simulation
        to hard disk to produce visuals from it
      - Checking on the machine that had the Agora Plantary disk simulation on it
        and that Jetstream machine is provin very difficult to access for some reason
      - Managed to work with Rishav Kumar who is Gustavo Almeida's student to send
        data over bluetooth from a wearable device to touch designer
    - - 1 tech test in person session at Crowder Hall
      - Created several new effects and tuned up the transitions between several of
        the fluid dynamics driven particle system
      - Took home a Shimmer IMU to practice wearable position detection inputs for
        touch designer
      - Will be meeting Gustavo's student Rishav next week to learn how to test these
    - - 2 tech test in person sessions at Crowder Hall
      - Had the opportunity to see how the material I've produced will actual look
        at the time of the performance
      - Lots of troubleshooting things that happened during the week
      - Recorded footage of Hayley from above for effects development
      - Spent time working on how to perform simple self-advection of velocity fields
      - This will allow a dancer to move their body and have their input spread to
        other particles on the screen they hadn't reached hopefully in a realistic
        way
    - - Had monthly meeting this week
      - presented my work for combining various effects using Hayley's floor dance
        recordings
      - then had development session with Kay on friday trying to get ready for the
        tech test week coming up where we are going to crowder hall in person to experiment
    - - Shot footage with Hayley last week
      - had meeting about visualizations that will be used for the floor dance section
        of the performance
      - Got 5 TB of data from northwestern researchers on solar formation
      - Wrote to Evan Schneider about galatic wind data but haven't heard back
    - - Met Kay last week for development session
      - Found that the fluid dynamics effect was broken because a single text file
        was in use by all the different touch networks so development broke the stable
        version
      - Asked researchers at Northwestern for their globus link to the 5TB dataset
        for the star birth
      - Tried compute shader network example for first time
      - Got the perspective, camera raw matrices to work
    - - Several long meetings with Kay working on this project
      - Decided where various Astronomy simulation data is going to fit
      - Made progress on recreating the effect used in the Manna lines video
      - Got 1 snapshot of data from Researchers at Northwestern and will try to work
        this into a touch designer animation
      - Figured out how to parse out the positions from the HDF5 data
      - have to reach out to Evan Schneider about galactic wind time series simulations,
        and start Globus setup for the Northwestern 5 TB dataset
    - - New scientist simulation data
      - Also met with Kay and discussed ongoing work related to lines
      - Set another meeting for this upcoming week
    - - Had monthly meeting last week
      - Discussion about grants that will be pursued by other members on the team
        following our eventual performance
      - Shared my updates about conversation with Gustavo on Sensors
      - Demoed my most recent visualizations using Touch Designer
    - - Wrote code to generate multisegment lines, but got stuck with how to apply
        this with multi line data also
      - Got curious about the scanline fill algorithms used to color the interior
        of geometries that we express with lines
      - Met with Gustavo from the sensor lab to discuss our options.
    - - Worked out how to use the Engine COMP to start processes in other threads
        so that we can keep our visualization performance high
      - This enabled me to implement a TCP client that receives raw bytes and converts
        them to a texture that we can operate on with the GPU
      - I believe this will help me bring the HPC into the project in bigger ways
      - Spent more time learning how to create multi segment lines in the compute
        shader
    - - Switched gears in development for this project to curves instead of particle
        systems
      - Reviewed videos that Kay sent to me for inspiration
      - Spent time reading Fundamentals of Computer Graphics chapters on splines,
        Hermite cubics, and Bezier curves
      - Made a proof of concept GPU compute touch designer example of 100^2 lines
        interpolated to a level of detail around 1000 segments per line
      - Somehow this still ran at 60fps? Not going to argue with the results.
      - Setup meeting with Gustavo the director of the new Sensor Lab to discuss the
        sensor options we will have access to for the show
      - Researched and implemented a system for switching between running Visualizations
        within Touch Designer so that we can script the whole performance from beginning
        to end without being hands on
      - This will be ultimately better because its still pretty likely that we would
        press the wrong thing at the wrong time and create a technical difficulty
    - - Big meeting this week
      - Also heard back from Brant Robertson and Evan Schneider that we can use their
        Galaxy outflow simulation visualization for our show
      - This is such a cool looking effect, I don't know where it will fit in yet
        though
      - Ran into some big performance problems with Touch Designer, but their most
        recent update appears to fix things
      - Made demos for meeting with Hayley and Kay
      - Will do more live outside testing with them next monday
      - Still stuck on creating simple propagation effects
      - Watched new series of videos by Stanslav Glasov, but he didn't go into the
        subject I was looking for on how to generate point/line network effects in
        touch designer on the GPU
    - - Rapid prep for presentation of work with our dancer Hayley Meier
      - Spent the week working on all of our existing visualizations trying to incorporate
        feedback from Kay
    - - Fixed Gasoline errors
      - Ran AGORA Disc example which is Isolated Milky-Way Like Disk Galaxy  using
        pthread over 10 cores (took 2 days to complete)
      - Created 2 visualization workflows for this within Touch Designer, Volumetric
        renderer, Instanced particle system
      - The volumetric renderer was a useful technique to brush up on but suffered
        from 3D texture size limitations in touch designer
      - The instanced particle system will be a much better approach requiring less
        pre-processing (no 3D texture to create, just parsing a binary snapshot from
        gasoline) and doesn't have visual artifacts produced at certain angles
      - visual results https://drive.google.com/file/d/1KFpEVgSEkSh4ZxSClgoLPJEL_Cp3m9ik/view?usp=sharing
        (instanced particle system) , https://drive.google.com/file/d/1DkSOdYlJpbWoZqremOAbLzGY9XB7OwTb/view?usp=sharing
        (volumetric renderer)
    - - Produced first MONOCOLOR inspired geometric scenes
      - Spent time in the week working with derivative community to troubleshoot performance
        of the geometry shader, it appears that the radeon pro vega 56 card on the
        imac pro may not be as performant as we hoped
      - developed a visualization of particles driven by a vector field based on the
        mathematical operations curl and divergence and the mouse input
      - Converted fluid mechanics particle system to be driven by kinect depth sensor
        mode
      - Experimented with James Wadsley's SPH program "Gasoline", but cannot get past
        an error which comes up on the Jetstream cloud instance when running with
        pthread on 6 cpu cores
      - So far no suggestions from the developers on the github issue I created for
        this
    - - Produced particle system visualizations driven by fluid mechanics coupled
        to user input (mouse or microsoft kinect)
      - Created first particle system with interconnecting lines using Kay's audio
        input
      - Discussing presentation options with Win Burleson for the top down dancer
        on stage section of movement 1
      - Found second researcher James Wadsley who may be able to provide solar formation
        SPH (smoothed particle hydrodynamics) simulations
    - - Got commercial license for Touch Designer
      - Exploring the differences from the free version
      - Learning to use the geometry shader section of the opengl pipeline to change
        the primitives that I operate on from points to lines
      - Exploring and recreating the techniques used in MONOCOLOR, Latent Space and
        Fulldome show by Marian Essl https://derivative.ca/community-post/monocolor-latent-space-and-fulldome-environment
      - Hitting snags related to renders that are higher resolution than 1280x720
      - Discussing options for movement 1's camera setups with Win Burleson
    - - Met with Kay to do Space Engine Flythroughs with the new version of the program,
        using its built in recorder
      - She thinks that we have all the space footage that we need for the show now
      - Met with Lewis Humphrey of Tech Launch Arizona to discuss what license we
        need for the programs we use in the project, he says we are commercial and
        Kay will try to raise money to pay for Touch Designer commercial license (600$)
      - Deadline for visuals of the first movement was Friday, but not everything
        was complete
      - Created a Trello to better track my progress and Kay's suggestions
      - New deadline is the 18th to finish part 1, and the first of april to have
        the second movements visuals
    - - Met with Kay to discuss development of particle system behaviors
      - Had a second meeting with her in the week to test out whether my iMac Pro
        desktop is faster than the resources she has, appears a mixed result
    - - Met with Kay, shot footage of various nebulae (horse head, carina, orion,...)
      - Finished storyboard for first movement of performance
      - Win Burleson met with Tech Launch to discuss Licensing, but haven't heard
        the results of that meeting
- - Virtual Nature
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: true
    status: active
    type: collaboration
    updates:
    - - Had phone call with Maria, mentioned progress being made towards bringing
        her talk to the university
      - Asked for clarification about her ideas related to hiring a student through
        the federal work study program to assist with the development
    - - Spent most of the last couple weeks working on learnigin to write Unreal Engine
        graphics programming code
      - This will be most important for leveraging HPC for visualizations when the
        time comes
      - Visual Studio is causing me a lot of grief and just found out that the hard
        drive that I was using for moving between different computers is probably
        broken
    - - Maria recently submitted the Epic grant, awaiting newsws related to this
    - - Discovered that the botanical gardens actually has an exhibit that is very
        close to what we are trying to develop in the spring
      - I will be reaching out to them in a little while to see if they are interested
        in becoming a community partner on the grant that Maria has submitted
      - Met with Omani and worked on more Unreal learning and development
    - - Omani recently started working on a audio localized Unreal scene - This means
        for other aspects of the virtual nature project we will be able to further
        immerse a user
    - - Met with Omani and set up Christine's old computer to be her work station
        going forward
      - She is going to practice using Unreal to simulate sound propagation in natural
        environments
      - I'm still going through a C++ Unreal course to make sure we can support Maria
        Harrington from UCF, and her collaboration with UA staff Greg Barron Gafford,
        and Aaron Bugaj of the Biosphere2
    - - Facilitated connection of Maria and Greg Barron-Gafford, setup meeting for
        them to discuss grant opportunity
      - Met with Omani to start talking about how the Niagara particle system tool
        works within Unreal
    - - Met with Maria Harrington this week and she's very excited to make contact
        with various researchers at the university who are interested in Virtualization
        of plants and the environment for their research
      - Will connect her with Aaron Bugaj and Greg Barron-Gafford for collaboration
        on her next "Epic" Grant submission
      - Will arrange a time to gather folks for a presentation of her work with the
        goal of identifying individuals on campus interested in longer term collaborations
    - - Officially made contact with Maria Harrington the researcher at UCF who gave
        the talk at SIGGRAPH on Virtual Nature
      - She and I are meeting next week to review her next grant submission to the
        games industry giant by the name of "Epic Games"
      - This company sponsored her work previously and she is re-submitting for the
        next phase of her grant
      - The idea is to become a vis consultant for one of the faculty on campus who
        can be her on paper collaborator for a Virtual Nature extension to the Southwest
      - Individuals to reach out to at the UA are as follows, Dr. Greg Barron-Gafford,
        Ellen McMahon, Dr. Scott Saleska, Bryan Black, Aaron Sparks Bugaj
    - - Will be testing Omniverse with the work machine this week to see if it's a
        viable way to allow the DCC Omani to work with unreal from her laptop
    - - Made video upload for Omani showing how we will take model data in the `.ply`
        format and make it visible in Touch Designer using a glsl compute shader
    - - Met and discussed material in touch designer related to creating particle
        paths
      - This was accomplished using post processing feedback TOPS and also with a
        "sliding window" of particle position history
    - - Watched presentation of Birds of a Feather at SIGGRAPH on "Virtual Nature"
        presented my Maria Garcia at UCF
      - Had the idea of trying to do collaborations with their group but centered
        on the southwest instead
      - Need to look for faculty who would be willing to participate in this collaboration
    - - Found an incredibly specific SIGGRAPH session of "representing nature" that
        we are going to go through and study for our uses
      - Will meet next week to go through it
    - - Met with Omani and discussed new idea to use photogrammtery to create the
        point cloud version of her scene
      - This means we need to automate the taking of high resolution photos from her
        unreal engine scene
      - Left that as a task for her to experiment with next week
    - - Created a new video upload explaining the process of sampling input textures
        and writing color to output texture
      - Met with Omani before that and did a recap of the syntax of GLSL
    - - Met with Omani last week and agreed upon meeting on tuesday's at 1:30pm
      - Will ask her for help converting her unreal scenes to bulk exportable models
        that I can employ the meshlab point cloud conversion on
      - Will be uploading compute shader instructional material to youtube as a stream
        every week on mondays
    - - Changing day of the week that I meet with Omani since her schedule is Changing
      - Haven't met up since we first discussed that change
    - - Met with Josh Levine and he helped me perform Poisson Disk Sampling in Meshlab
        to translate a meshed model into a sampled point cloud as one step
      - Using PyMeshlab this will be an automatable process too
    - - Met with Omani and learned to create squares on the screen via the compute
        shader
      - Had several breakthroughs related to the Mesh-> Point Cloud Conversion
      - Wrote a little python program to sample obj image textures at 3D points for
        each geometry face and write that out formatted as .xyz
      - Next need to try this on some of Omani's plant models if she can export them
        from unreal
      - Got thumbs up from Ash about changes to the internship involvement document,
        now need to find out what's needed from me before the 7th of June
    - - Met with Omani and talked more about how to use Touch Designer for compute
        shaders
      - Corresponded with Ash about internship arrangement, haven't gotten all the
        details kinked out but a working document is here https://drive.google.com/file/d/1a4dsJT1ZWRVSsP7CJB7bCF6RqsKClblq/view?usp=sharing
    - - Didn't connect with Omani last week because of a time conflict on her end
        with building maintenance
      - Started working on a Proof of Concept for helping to convert models into point
        clouds
      - Wrote a js triangle sampling program which will be used when loading pixels
        from an image that is used as a mesh face texture. https://trianglesampler.baylyd.repl.co/
      - Had a big meeting with Ash, Victoria, and  Aaron and it sounds like I'll be
        working with Tech Core students this summer on the Biosphere2 collab
    - - Met with Omani, and started in a new direction
      - We will base our work on her photo realistic nature scene
      - Take models from it and convert into the 2 types of assets that I've identified
        in the tree-hugger video (pseudo lidar, dynamic point paths)
      - She asked to specifically learn more about shaders so I've started teaching
        that material using the shadertoy program, I should mention to her that touch
        is free for non commercial and perhaps it will run on her laptop
      - I have to decide what makes sense to do with the results that I already have
        from the Open Root Sim if we go in this direction
      - Ash Black was interested in this project in our meeting, perhaps find a way
        to make this into work that his students can take on
    - - Fixed issues with converting vtu to obj!
      - Apparently issues stemmed from using 0 based counting in the obj face indices
      - Gave Omani  full folder of simulation results to work with
      - Started email chain with Developers at Open Root Sim about how to change the
        simulation parameters to create different plant root systems
      - I believe we won't get very far with this because they only focus on a few
        model plants and nothing bigger than a corn plant
      - Looked into L systems for generating artificial root structures, but it would
        be great to get scans of some much bigger plants
      - Omani started working on the VFX of animating particles traveling along the
        outsides of the mesh, but I think that's going to take time
    - - Still have significant issues converting the .vtu data to .obj
      - Even branched out to looking into the .ply format which I could then skin
        with blender later on
      - Small breakthrough related to discovering Paraview (which opens .vtu without
        issue) can export to gltf, but so far there isn't any clear way to automate
        this process
      - Even the pyparaview doesn't include much information about how to do this.
      - Spent more time looking into using the python VTK package to export
      - Researched writing L-Systems for synthetic branching strutures, and created
        a Rust Package to generate the L-System Axiom, and then wrote a parser that
        places points along the structure
      - Can view the results in Meshlab by importing .xyz formatted point cloud https://drive.google.com/file/d/1pJUg8obP_yyV05M8ESPK11xaVZUsyNnI/view?usp=sharing
    - - Met with Omani
      - Started working on the vfx out of unreal engine
      - There doesn't seem to be many easy ways to control the speed at which some
        effects propagate
      - Uploaded Open Root Sim to Jetstream and ran a bunch of sample simulations
        to give us root meshes to work with
      - Spent most of my time re-writing the .vtu to .obj script
      - Facing a really annoying bug in which there's an extra face at the end of
        each triangle strip, but there's no clues as to how to make sure I'm indexing
        the correct number of vertices
      - Once I have this worked out I can send things along to Omani
    - - Met with Omani
      - She was able to present some of her previous examples of biological scenes
        using unreal engine, super impressive
      - Plan is to make use of unreal's effects bound to wireframe models
      - I'll now try to produce a large number of converted meshes that she can try
        to import for running various types of VFX on
    - - Produced multithreading rust program that can do our "point to texture" conversion
      - from here I generated a brief example animation to demonstrate how we can
        use these textures with particle system visualizations
      - https://drive.google.com/file/d/13mIKrDNFu6SYvQSZCQxqFJnntc7HCl0y/view?usp=sharing
      - Aaron Bugaj notified us that the green fund is no longer a viable option for
        funding the media vis walls grant
      - Treating this year as a prototype of the program for working with students
        that have Biological System visualizations they want to do
    - - Started working on exploring unreal engine offerings related to our eventual
        visualization
      - Omani says the mist engine might be what we need for particle systems?
      - Created https://repl.it/@baylyd/quadtreeroot#script.js ,https://quadtreeroot.baylyd.repl.co/
      - Started working on Numpy method of calculating Luminosity of texture based
        on the distance that pixel lies from the nearest root point
      - Will use this texture for particle movement against or with value gradient
      - Ran into memory limit for certain sections of the root system on a Jetstream
        allocation
    - - Omani now has DCC status, which is great
      - Met and she got unreal engine installed on the machine that is more stable
        than the laptop
      - She's going to continue to brush up on her python
      - I'm going to try to create the first 3D texture of our first root so that
        we can use other programs than p5.js to do some of our first animated visualizations
    - - Succeeded at parsing the converted .vtu file into a mesh surrounding the points
        of the root simulation
      - Started a small example openprocessing sketch to explain the work to others,
        and myself at a later date
    - - Made more progress on understanding the .vtu file format and its layout of
        data, useful for visualizing the results of Open Root Sim
      - Began work with wireframe visualization of Zea Maize root development
- - Collaboration With Techcore'S Summer Internship
  - contacts: Ash Black, Victoria Ogino, Aaron Bugaj, Joost(?)
    dates: null
    description: null
    links:
    - https://hackmd.io/xi_m4Kj6QDenBR3ZAfN3iw?edit
    - Project folder on gdrive: https://drive.google.com/drive/folders/13v9QfUFVjQD-x7dh8chQZFvAxmy5zmx6?usp=sharing
    - videos: https://drive.google.com/drive/folders/18tT28oLiXFH1wH8NGUwSU1K0URyzJq9o?usp=sharing
    - Pecha Kucha style presentation: https://docs.google.com/presentation/d/1n6ggKJ-oG7fqHfVsLMnuYkAVh095B1RA7EcNx1RQ12M/edit?usp=sharing
    - presentation video: https://www.youtube.com/watch?v=EiQ9S5lNbA8
    newUpdates: false
    status: complete
    type: collaboration
    updates:
    - - Just had the final presentation for this project
      - Worked with Trevor to render out the last of the videos that we are using
      - Got stuck trying to use gdrive for streaming our 4k 360 videos, apparently
        we went over the download cap set by the google drive api
      - Had meeting with researcher Laura Meredith to make sure that our work was
        in line with her standards for the experiment
      - Worked with Nick Vinas to finish up his animated Vega Lite Graphs
    - - Completed incorporating the xorshift PRNG algorithm into the compute shader
        and it works much better for random uniform 3d motion for gas particles
      - successfully incorporated Laura Merediths CO2 and N2O data into the visualization
        such that the concentrations in the data reflect in the transparency of the
        total number of particles
      - Generated a background population of particles to suggest the existence of
        gas particles not just at the locations where the probes were, but without
        apparent changes in concentration
    - - last week the students had the chance to go to the biosphere 2 rainforest
      - Also we found out that we aren't going to have the new lidar data coming from
        Joost so we are changing to supporting a photogrammetry worflow
      - Trying to keep the interns busy working on things that fit with their respective
        interests, but the creative track students are a little hard to keep busy
    - - Switched gears to using Unity for our output videos
      - Lots of learning that I have to do in order to make this work
      - Figured out how to create custom scriptable render features/passes, compute
        shaders/buffers, material property blocks, indirect GPU instanced Mesh Draw
        calls, and some of their ShaderLab /HLSL code
      - The performance of graphics in Unity is much better than I expected and it
        might be a program I use again after this project
      - The other students are doing well, and have  figured out how to ask eachother
        for help where expertises apply
      - Farabi is doing great work on the Website and the WebVR, Trevor is learning
        all the low level graphics I can throw at him, Samantta is doing great work
        with Adobe Illustrator
      - Nick and Jai both need more direction on what should be done with Python and
        the modeling, but I think that's just until they find their feet
    - - Had first meetings with my interns, Jai Stellmacher, Trevor Hoshiwara, Samantha
        Garcia, Nick Vinas, Al-Farabi Muhtasim
      - Nick and Jai will be the data cleaners and 3D modelers
      - Samantha is going to be the creative/aesthetics lead
      - Trevor will be the glsl touch designer assistant
      - Farabi will be doing the front end development for the final outputs
      - We will be producing 360 video content showing rainforest lidar scans with
        animated research data of NO2 and CO2 recordings via probes at one location
        and different depths over time
    - - Several meetings with Ash and Aaron last week
      - Met researcher Laura Meredith from biosphere2 rainforest who's research data
        we will be using
      - Had meeting for me to lay out the way the output is going to be created in
        the internship
    - - Started work with Trevor
      - update the working document with material related to how I want to approach
        the internship outputs
    - - Met Ash, Aaron, and Trevor Hoshiwara to discuss the first steps of the project
      - Getting a meeting with Joost and Laura to discuss their actual data that we
        will combine with the lidar material
      - Tentative plan is to aim for generating 360 video that can be watched in a
        headset
      - This will allow us the most freedom for the steps involved in combining the
        static structural data with the dynamic scientific data
    - - Went through  my email and found the first Rainforest biome lidar point cloud
        data set
      - Haven't heard from Joost and Aaron about whether I'm allowed to copy the file
        to my own systems to attemp to visualize though
      - Aggreed to have a kick off meeting with Ash and other members of this collaboration
        on June 3rd
- - Jason Hortin Holographic Dance Graduate Project
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: upcoming
    type: consult
    updates:
    - - Helped Jason connect with Bryan and Tech Core with idea that his holographic
        dance project could be part of the Tech Core summer internship
      - Last I heard he had connected with Bryan to do some volumetric video capture
        in the Digital Humanities new center
    - - Wrote another email to Jason and mentioned he should get his project into
        the running for projects listed in the tech core summer Internship
      - He also reached out to Dr. Bryan Carter recently to see what the options were
        for doing volumetric capture footage recording
      - Sounds like more will happen in the fall because he's going back to Chicago
        for the summer
- - Radiology 1St Year Resident Carl Sabotke
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: complete
    type: collaboration
    updates:
    - - Sending out introductory emails for Carl to Win Burleson, Ash Black, Bryan
        Carter, and Jenn Nichols
    - - Met Carl for the first time with Chris Reidy and Blake on Thursday
      - Said I would introduce him to all the folks that I know of who might be able
        to help with his AR visualization needs
      - Probably can't do much myself because I'm not familiar at all with the Hardware
        he's trying to use (Hololens)
- - Remote Visualization Infrastructure Development spring 2021
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: complete
    type: infrastructure
    updates:
    - - Spent part of last week learning about "transfer encoding chunked" messages
      - These make it possible for a http client to continuously receive data using
        a ReadStream and may be a method for sending streams of data from HPC when
        on a VPN session.
      - Tried to get started with Omniverse on the Exxact system that I was a test
        driver for, but got stuck waiting for approval to use their docker containers
    - - Just got access to A100, will attempt to use it for some Omniverse testing
    - - Attended the RMACC conference and saw quite a few really interesting presentations
        featuring in-situ visualizations using HPC
      - Notable speakers were Kenny Gruchalla, Steve Cutchin, George Takahashi, Philip
        Black & Patrick O'Toole
      - Corresponding with Gianluca about nvidia related things (Numba graphics, and
        vulkan setup things)
      - Learned the term Pixel Streaming, which means displaying somewhere different
        than the compute
      - Started using Jax as a way to make use of GPU's from python Linear Algebra
        code, could be very useful for some basic headless renderings
    - - Had meeting with our Nvidia reps Bruce McGowan, and Gianluca Castellani
      - Asked them all my vulkan questions, and they reported that we should actually
        see that we have support?
      - Adam helped uncover that we are missing the /etc/vulkan files related to the
        icd.d and implicit-layers.d folder
      - This suggests that the installation on Ocelote may have been unsuccessful
        in some regards
      - Bruce and Gianluca are now helping me try to figure out the issues related
        to Numba and Jax to see if I can run simulations through these
      - They recommended that I try to attend RMACC which is a HPC conference happening
        next week which may have interesting examples of sci vis on the HPC
    - - Met with Chris to discuss vulkan support
      - He's still working on NoMachine configurations,
      - Perhaps going to setup the vulkan 1.1 when the x server is live and see if
        I get different errors
    - - Met with Chris and discussed remote visualization options
      - Sounds like we should look into other tools that support rendering vulkan
        windows that aren't just NoMachine, still need to get to the bottom of that
        conversation with Chris
      - Discussed Omniverse usage
      - Tested Vulkan on each HPC and each have the same Invalid Driver message
      - Created a vulkan issue on their tracker https://vulkan.lunarg.com/issue/view/608706d05df112e5e0a11a5a
    - - Chris notified me that NoMachine was installed on i18n16, but there's firewall
        issues with port 4000 so I wasn't able to test this out
      - Met other members of the Omniverse team who are willing to help me when I
        run into issues with remote visualization for my projects
      - Tangentially related, I figured out how to render raw byte streams of data
        over TCP in the Touch Designer program which may be a personal method for
        doing remote visualization from simulations running on the HPC
    - - Met Peter Messmer of Nvidia's HPC remote visualization division at Nvidia
        GTC
      - Discussed several projects that I want to leverage their tool Omniverse for
      - Got positive feedback that streaming results to displays like CATalyst's big
        display walls or even Biosphere2 should be included in their  Omniverse Beta
        release that's out now
      - Met quite a few other individuals who will probably be able to assist with
        this going forward
      - Will be testing using Exosphere and my own local machines before looking into
        HPC -> Catalyst
    - - Created brief statement for Blake to submit to the All hands accomplishments
      - Met with Chris Reidy to discuss the newest developments
      - Gave a demo showing the 1.5 million particle attractor using Jetstream Exosphere
        and the NoMachine remote desktop
      - Started working on how we can use Vulkan on the HPC
      - Chris said he would try to install NoMachine on i18n17 so that we can test
        whether that fixes the `Incompatible Driver` error when running `VulkanInfo`
    - - Big breakthrough learning more webgpu with the Rust systems programming language
      - Now understand enough to create some of the simple visualizations I've produced
        in Touch Designer
      - This means a significant improvement in performance because its written almost
        from scratch in gpu modern (vulkan not opengl) graphics code
      - Ran demonstration on 3 different machines of particle system where the mouse
        was an attractor
      - On new desktop was able to run 15 million particles close to 60 fps https://drive.google.com/file/d/1yyVqGcrgrW1rjqE8DPML8LDsJVetj1h1/view?usp=sharing
        Using laptop as nomachine display for exosphere cloud instance with 1/4 of
        V100 was able to run 1.5million particles in realtime
      - Performance dropped on exosphere using 15 million particles
      - Video links https://drive.google.com/file/d/1JbMX0GnxqVZgED8vcfoNEKGmEeYaCPld/view?usp=sharing
        (1.5 million), https://drive.google.com/file/d/1zg-fUDU4cYLmW-bksi3GBwplzqWnTBj7/view?usp=sharing
        (15 million)
    - - Finished converting a working graphical simulation of flocking points
      - This is a quintessential n-body update algorithm that would be a good indication
        of the workflows capability for running compute tasks on the gpu and rendering
        the results in realtime
      - Will be testing this out on exosphere this week
      - If that's successful then I will work on getting the example setup on our
        HPC
    - - Succeeded in using Vulkan via NoMachine remote desktop running on subdivided
        V100 Nvidia GPU
      - Created a screen cast video explaining the setup steps and sent to Jeremy
        Fischer in charge of NSF's Exosphere
      - He's very excited and is going to use the video to create documentation for
        users interested in my workflow when Exosphere is released to the public
    - - Worked on Jeremy Fischer's exosphere Nvidia JS Ubuntu 18 instance
      - Worried that their Nvidia V100 gridded VGPU isn't going to actually work for
        Vulkan which will limit the access to modern graphics development that researchers
        can do in their instances
      - Uncertain whether this is going to predict whether Vulkan will work with our
        V100s and K80s since they aren't currently being subdivided into virtual GPUs
      - Will be trying all of my steps on Exosphere outside of singularity containers
        this week to determine if singularity is the problem
    - - See whether I can use the singularity containers in the HPC testing environment
        that Chris has setup on Ocelote
      - If that works out, transfer the containers to the exosphere GPU instances
        I have from Jeremy Fischer and test there
    - - Successfully configured new desktop to work as build environment for the singularity
        containers that will have remote visualization/advanced graphics capability
      - Did test with nvidia/vulkan (openGL's successor) and was able to get graphical
        windows from the vkcube, and all of the rust gfx-hal demos
    - - Received new desktop machine with Nvidia card inside
      - Started trying to develop containers for use with HPC remote visualization
      - Windows Subsystem for Linux 2 is stuck with certain Nvidia Utils like nvidia-smi
- - 3D & Vr Retrofit Azlive
  - contacts: null
    dates: null
    description: null
    links:
    - https://rtdatavis.github.io/#retrofitAZLIVE
    newUpdates: false
    status: complete
    type: collaboration
    updates: null
- - Discussion about Social Network Analysis Visualizations
  - contacts:
    - Leih, Rachel - (rleih) <rleih@arizona.edu>
    - Gildersleeve, Rachel - (ragildersleeve) <ragildersleeve@arizona.edu>
    dates: null
    description: null
    links:
    - NA
    newUpdates: false
    status: complete
    type: consult
    updates:
    - - I received a message from Jean McClelland mentioning there are two faculty
        lookin for assistance with Visualizations for their project that involves
        Social network analysis
      - Set meeting up for this upcoming Tuesday
      - Leih, Rachel - (rleih) <rleih@arizona.edu>
      - Gildersleeve, Rachel - (ragildersleeve) <ragildersleeve@arizona.edu>
- - Advice For Thesis Defense Visualizations, Sabrina Nardin
  - contacts: null
    dates: null
    description: null
    links:
    - NA
    newUpdates: false
    status: complete
    type: consult
    updates:
    - - Answered follow up questions about her new stream graph over email
    - - Met with Sabrina to discuss her dissertation defense's visualizations
      - Her data is visualizing 8 different violent events from Italy's history in
        the last 50 years covered by 3 different newspapers
      - Most of her questions were about how to improve her existing approaches so
        I laid out the foundation of "task abstraction" taught to me by Joshua Levine
        using Tamara Munzner's design theory for visualization
      - Shared resources with Sabrina and recommended a few changes, but overall tried
        to equip her with the ability to critique her own work
      - Mentioned other best practices such as sharing her visualizations with as
        many other people that are like her target audience as early as possible
      - Has asked for a follow up based on changes incorporated since the meeting
- - Argonne Gpu Hackathon
  - contacts: null
    dates: null
    description: null
    links:
    - https://drive.google.com/file/d/1yzTyizIMLxRXabMTQgk7KzECsVYHrXOf/view?usp=sharing
    newUpdates: false
    status: Complete
    type: infrastructure
    updates:
    - - Got email that due to limited space our team and its proposed project weren't
        admitted
      - it's ok, I extended a webgpu rust boid simulator example and have it working
        on exosphere so perhaps I got something similar to the experience of the hackathon
        from that
      - https://drive.google.com/file/d/1yzTyizIMLxRXabMTQgk7KzECsVYHrXOf/view?usp=sharing
    - - Should hear from the organizers about whether we are admitted this week
    - - Blake asked if he could join, so I spoke with organizers again
    - - Dima asked if he could join the team so I wrote to the organizers to update
        our roster and they allowed it
    - - Submitted an application to participate in the Argonne GPU hackathon which
        takes place at the end of april
      - Team mates are Sara Willis, Chris Reidy, and Abby Collier
      - Our project is to produce a gpu accelerated flocking simulation that we can
        remotely visualize from a demo created to run only on a single cpu
      - Waiting on updates about our application
- - Autamus Web Interface
  - contacts: null
    dates: null
    description: null
    links:
    - https://rtdatavis.github.io/#autamus_interface
    newUpdates: false
    status: complete
    type: infrastructure
    updates: null
- - Bio5 Virtual Reality Tour
  - contacts: null
    dates: null
    description: null
    links:
    - https://rtdatavis.github.io/#bio5-vr-tour
    newUpdates: false
    status: complete
    type: collaboration
    updates: null
- - Bryan Carter Photogrammetry
  - contacts: null
    dates: null
    description: null
    links:
    - https://sketchfab.com/3d-models/churche-maze-photogrammetry-d0767af08a5d498fb960efe3ac83385f
    - https://drive.google.com/file/d/11E5_912vW6kuPBxcujGKs-UQD9S3jA-H/view?usp=sharing
    newUpdates: false
    status: complete
    type: consult
    updates:
    - - Tayvien Ahmaud William's photogrammetry using Apporto and Metashape completely
        worked!
      - Very impressed with the final result, and I hope I can ask him how he managed
        to work around the timeout limit on Apporto
      - Full version of the model can be found at https://sketchfab.com/3d-models/churche-maze-photogrammetry-d0767af08a5d498fb960efe3ac83385f
    - - Met with Tayvien and tested out whether Metashape is going to be able to handle
        the number of images he has
      - Sounds like there will probably be a need to use HPC just because the runtime
        is a bit too much for apporto?
      - Should reach out to him and ask when his finals are over so that we can meet
        again and get situated on the HPC
    - - Meeting Tayvien who works with Bryan on Thursday this week
      - Will spend some time getting him started with apporto and metashape to test
        things out
    - - Met Bryan last week for a Metashape Demo
      - Talked about how access might work with the HPC
      - Need to explore the setup of Tyson's license inside the HPC environment
      - Discussed using UA's Apporto as a means of getting his studentt up to speed
    - - Tyson and Bryan were able to meet, but I could attend
      - Have a meeting with Bryan set for first week of March to discuss HPC and Metashape
        workflows
    - - Met and discussed array of different upcoming potential partnerships with
        the new center for digital humanities.
      - Potential project to explore using the Nvidia Omniverse for streaming the
        results of HPC calculations/render to center for digital humanities
      - Discussed meeting with Tyson to get started learning to use Metashape for
        his photogrammetry needs
      - Discussed using jetstream allocations instead of their AWS accounts to provide
        on demand access to media for their research projects from user's phones (didn't
        go into details about this)
- - Covid Retail Mitigation Web Scraping
  - contacts: null
    dates: null
    description: null
    links:
    - https://rtdatavis.github.io/#retailscraping
    newUpdates: false
    status: complete
    type: collaboration
    updates: null
- - Force Directed Biochem Networks
  - contacts: null
    dates: null
    description: null
    links:
    - https://rtdatavis.github.io/#biochem-networks
    newUpdates: false
    status: complete
    type: collaboration
    updates: null
- - Judging The Data Visualization Challenge
  - contacts: null
    dates: null
    description: null
    links:
    - https://ual-odis.github.io/data-viz-challenge/
    - https://arizona.figshare.com/search?q=visualization+challenge
    newUpdates: false
    status: complete
    type: community
    updates:
    - - Completed this, my favorite entries also were the ones that won so at least
        I was judging along the lines of the others
    - - Two more entries of 15 to judge, had a few issues with the rubric in the process
      - Likely will circle back just to make sure I'm executing my judging process
        consistently on some of the more nuanced scoring categories
    - - Started reviewing the entries, on track to finish this before the 28th
    - - Was notified by Jeff that this is the week to review the metric, and next
        week we will begin reviewing submissions
    - - Planning on approaching a few departments with the offer of a data visualization
        introductory piece for their grad students
      - Will start with my home department of Neuroscience and perhaps Speech Language
        and Hearing where I've done quite a bit of work already
    - - Met with Jeff Oliver and Kiri Carini to host a peer review session
      - Several individuals showed up with questions about whether their projects
        qualify
      - At the end we discussed options for Kiri to join our Data visualization roadshow
        for departments interested in geospatial visualizations
      - Reached out to Stephen Rains in the Department of Communication to offer a
        presentation for his upcoming Computational Social Science (CSS) mini conference,
        but he said the sessions were all full and they didn't need more presentations
      - He did send our email offer for intro to data visualization presentations
        to Joe Galaskiewicz who's the director of the CSS certificate program
    - - Spoke with Jeff Oliver about helping with a peer review session for individuals
        participating in the data visualization challenge
    - - Started going over the website for information related to my judging duties
    - - No updates this week
- - 'Migrant Forensic Empathy Project: A Digital Borderlands Grant Initiative'
  - contacts: null
    dates: null
    description: null
    links:
    - https://mfemigrantdeathmap.baylyd.repl.co/
    - https://devinbayly.github.io/digital_borderlands_conversion/src/mesh_test.html
    - https://devinbayly.github.io/digital_borderlands_conversion/src/index.html
    - https://hackmd.io/Qo_HmwmwSTG-QPYVTJl0Lg?view
    - https://osf.io/v9swc
    newUpdates: false
    status: complete
    type: collaboration
    updates:
    - - Turned in my documentation and project overall to Jason, and will await any
        students he brings forward to pick up where I left off
    - - Spending all my time on this writing the final deliverable wiki and OSF project
        page
    - - Met with Jonathan, and heard that his grants weren't accepted by the library
      - Discussed current state of the project and got his go ahead to write a wiki,
        and then transition the project to closed
      - Will be including sections around what remains to be created and some tips
        about how a student might proceed with such things
      - https://devinbayly.github.io/digital_borderlands_conversion/src/mesh_test.html
      - https://hackmd.io/Qo_HmwmwSTG-QPYVTJl0Lg?view
      - Created OSF project to house additional files and the wiki so that there's
        a DOI associated with the work for when Jonathan publishes about it https://osf.io/v9swc
    - - Sent email to Jonathan to update and request meeting next week
      - Will be ending active development in 2 weeks
      - Created bi directional transport between shrine and the new 3D portal
      - Landscape teleportation via clicks on the mesh
      - Crosses now listen for click events, and create text above featuring whatever
        information we have about the migrant the cross belongs to
      - Crosses now populate/remove incrementally based on the location of the camera
      - Generated UV rotation so that the ground texture doesn't look like 100% the
        same pattern on all parts of the landscape
    - - Lots of time going into this to try to wrap things up by the end of May
      - Unfortunately I've hit a considerable obstacle that the mechanism I was counting
        on doesn't work for constraining the camera motion to the landscape mesh
      - I've spent a significant amount of time trying to research alternatives and
        implementing my own solutions and I'm coming up empty
      - I have posted to SO about this and hopefully that will get some attention
    - - Time to really push on this project
      - discovered that the "shader:flat" method can help eliminate artifacts introduced
        from a renderer that's struggling with shadows on a large mesh
      - rewrote the populate and query side of the quad-tree in non recursive algorithms,
        and dealt with bug that was running my machine out of memory in the development
        of this
      - New ideas about how to integrate the shrine into the landscape in a modular
        fashion
      - Will probably make a shallow quadtree for querying the user position every
        5 seconds to see if we need to render more or less crosses in the environment
    - - Got answers about the visual artifacts on SO, sounds like it is a camera parameter
        issue, hopefully I can still resolve it using a shadeless environment texture
      - Re-implemented the quad tree algorithm to allow for subsection overlap, unfortunately
        this caused my recursive algorithm to stack overflow when I put greater limits
        on the number of elements per region
      - Re-learned some more advanced methods in rust that will allow me to write
        the "populate quadtree" and "query" algorithms without recursion
      - For the mean time, just started long running task and processed cross placements
        much more accurately
      - It's pretty heartbreaking to see the crosses that are right next to each other  https://test-cross-placement.baylyd.repl.co/
    - - Created realistic scaling version, but found that there's visual artifacts
        on the faces far from the camera, created SO issue for this but I imagine
        I will just have to downsize the mesh and alter other aspects to make the
        landscape scale feel appropriate
      - Spent a while re-writing the cross placement Rust code, so that the quad tree
        regions feature overlap
      - This ultimately makes the program less performant but the results are much
        more accurate
      - Performance benchmarking by isolating each individual entry of the scene and
        checking the fps
      - Learned that there doesn't seem to be any one piece causing performance issues
        which is comforting
      - Reached approximately the halfway point of allotted time for this project
        and there's still some big aspects I still have to implement
    - - Big push on this project this week
      - Wrote a Rust program which uses a quad tree structure to organize the 3d points
        of all our landscape vertices
      - When we convert the Medical Examiner's office gps data to mesh coordinates
        we can then query the x,z position of a cross against the quad tree and get
        optimized calculations for the y coordinate where the cross should be placed.
      - I then created a demo scene to show the mesh and the crosses placed in the
        landscape https://test-cross-placement.baylyd.repl.co/
      - <10% of crosses placed above mesh in the air, so this next week I'll have
        to do some test cases with a smaller mesh, and known placement points.
    - - Got data on lat-lng for migrant deaths along the border from Jonathan
      - Created simple webmap of this to help understand how many crosses may show
        up in the Aframe scene
      - https://mfemigrantdeathmap.baylyd.repl.co/
      - Unfortunately the previous student has cropped some of the digital elevation
        map (DEM) from its original bounding box so I can't make a function to map
        between the lat-lng range in the data and the x-z coordinates  of the landscape
        model
      - I had to recreate a Marching Cubes workflow to process the a URL to download
        a DEM into a 3D mesh
      - I'll be using the bounding box of this to convert the gps locations of the
        deaths into positions at which I need to place cross models in the landscape
    - - Recreated scene at larger scale
      - Troubleshooting some of the performance issues that are starting to drop us
        to 12fps
      - Experimented with raycasting for interaction with certain objects in the scene,
        performance droped to 5fps from this
      - Took screenshots to send to Jonathan with an update
    - - Heard back from Jonathan that the grants have been submitted
      - He put in a second grant to "recharge" for my time if comes through so we
        will probably have to talk about the finances of that again
    - - Generated first draft of content converted to Aframe https://drive.google.com/file/d/1GAPmOVXS1lgGaoFf_8nSHtGIbJh2XRxk/view?usp=sharing
      - Some issues listed here in my development issue tracker https://github.com/DevinBayly/digital_borderlands_conversion/projects/1
    - - First official week of development for this project.
      - Spent most of the time working on retrieving and modifying the landscape model
        of Southern AZ near "Oregon Pipe National Park"
      - Experimented with the Meshlab python api for programmatically editing/filtering
        large point clouds into meshes
      - Drafted process for systematically importing existing assets into Aframe
    - - Picked up an oculus rift from the library that I can use for 2 weeks
      - I'll try to record some of my previous unity build reviews so that I can refer
        to those as I start the Aframe conversion this way I don't have to rent the
        hardware multiple times
    - - Met with Jonathhan Crisman to get more details about the files that came with
        his folder of student work
      - Looks like i'll need to borrow an oculus in order to demo his scenes so that
        I can see what to create
      - Will reach out to Jen Nichols, and Jonathan  again to get help with this,
        Bryan Carter is where Jonathan got his temporary set so I might start there
        also
    - - Meeting with Jonathan Crisman to discuss beginning questions for converting
        his existing application to the Aframe WebVR platform so that future modification/maintenance
        is easier.
- - Mt. Lemmon In Your Pocket-Creating A Virtual Reality Tour
  - contacts: null
    dates: null
    description: null
    links:
    - https://rtdatavis.github.io/#GIS_week2020
    newUpdates: false
    status: complete
    type: workshop
    updates: null
- - Neuro Choropleth
  - contacts: null
    dates: null
    description: null
    links:
    - https://rtdatavis.github.io/#neuro-choro
    newUpdates: false
    status: complete
    type: collaboration
    updates: null
- - Oyster Vibrio Literature Review
  - contacts: null
    dates: null
    description: null
    links:
    - TBA
    newUpdates: false
    status: complete
    type: collaboration
    updates:
    - - Emily had her defense last week, will now reach out to see if she is happy
        with the project and wants to call it done
    - - Met with Emily and created a leaflet webmap with her lit review GPS data
      - Will meet maximum 2 more times to help her get the website ready for her paper
    - - Emily had to reschedule to Tuesday
    - - Sent email to check on status of this as its been 6 months since their last
        update
      - Setup meeting this friday to discuss her needs and advise about data visualization
        techniques
- - Presentation For Civil Engineering Department
  - contacts: null
    dates: null
    description: null
    links:
    - https://docs.google.com/presentation/d/15Z9zcxU4vIIgFPnKEcaGv9GH7JtjNdx4Xpnjec0EzEc/edit?usp=sharing
    newUpdates: false
    status: complete
    type: workshop
    updates:
    - - Met with Jeff
      - Debriefed Civil Engineering presentation
      - Created action items planning future presentations
      - Discussed Department of Sociology and Anthropology presentation with Kelsey
        Gonzalez for the 16th of April
      - Created planning document with our debrief thoughts, presentation opportunities,
        and content relevant to each individual audience
    - - Met with Jeff Oliver, ran through our presentation to practice
      - Discussed changes, sent the presentation to Joshua Levine to double check
        the attributions for material from his slides
      - Presented to the graduate civil engineers seminar
      - Jeff mentioned that it went so well that he wants to "take it on the road"
    - - Decided on the examples I'll be showing
      - One finite element method visualization of stress forces spreading through
        a model
      - Second one is going to be an Aframe scene of an intersection with car models
        steered by data coming from "CityFlow" the open source traffic simulator
    - - Met with Jeff Oliver last week to build outline for presentation.
      - Got OK from faculty Josh Levine to use material from his slides for the "why
        we do data visualization" and "best practices" portion of the presentation.
- - Resbaz Organizer And Workshop Provider
  - contacts: null
    dates: null
    description: null
    links:
    - https://hackmd.io/-XS5Mqh8TA2EHjTHCQ_4tw
    newUpdates: false
    status: complete
    type: workshop
    updates:
    - - Presented my Observable notebook workshop last week
      - Had about 10 attendees at the end of the week at 3pm to 5pm which was great
      - Got feedback that I went too fast and that the material wasn't beginner friendly
        so I'll probably re work the presentation notebook somewhat
    - - Met Alex last week and worked on our res baz presentation
      - Extended our notebook so that there's a collaborative live survey built into
        the javascript
      - Fancy way to demonstrate the capabilities of the notebook
    - - Met Chinmay one more time
      - Heard back from Kelsey about what the listing should be for the videos (public
        unless the instructor says otherwise), I think we need to make sure that's
        spelled out for folks attending workshops
      - Got our Observable workshop listed in the webpage
    - - Met Chinmay and we discussed some final todo items
      - Testing our links next week and publishing them to the website team
      - Noticed that my workshop still isn't listed on the resbaz page so I'll have
        to get in touch with Alex about that
      - Blake volunteered to perform the uploads to youtube so that's helpful, still
        haven't heard from Kelsey about whether our unlisted approach is going to
        be the correct way to go
    - - Built workshop Observable notebook
      - Created observable notebook conversion of Kate Isaac's Vega-lite data visualization
        workshop
      - Discussed with Blake what the process would be for adding videos to the resbaz
        youtube channel, it sounds like he'll take that on just because it sounds
        like access is a tricky subject
      - Setup next meeting with Chinmay for following wednesday
    - - Met with Chinmay Joshi
      - Discussed our todo items for getting the zoom links created
      - Discussed scripts to provide the helpers and the workshop presenters so that
        things go smoother than the last time that Chinmay attended
      - Discussed new approach to the recordings, and double checking on Accessibility
        strategies with Kelsey Gonzalez
    - - Met with Alex and planned our workshop
      - Will send outline to Ian Johnson who works at Observable and see what he thinks
      - Meeting with other Zoom and Youtube Coordinator Chinmay at somepoint to practice
        zoom and youtube upload
    - - Meeting with Alex Next week
      - More planning conversations on Trello regarding zoom usage
    - - Planning on trello
      - Discussing the needs for Zoom and how to use gather.town for community sessions
    - - Got access settings set correctly for Alex to modify the document
      - Discussing individual tasks with Chinmay for the Zoom and Youtube coordinators
    - - started working with Alex and Kate on a google doc to organize our plans for
        introducing ObservableHQ at the resbaz this year
      - connected with Fernando Rios' buddy Ian Johnson who works at Observable
    - - Met with rest of organizing group
      - Put myself down as one of the two youtube/zoom coordinators since that role
        is similar to how I'm helping the Women's hackathon
      - Pitched an ObservableHQ visualization workshop and Alex Bigelow is going to
        coteach and help with development
    - - Organizer meeting this week to discuss what workshops to offer, and other
        administrivia
- - Social Vr Museum Capstone Student Project
  - contacts: null
    dates: null
    description: null
    links:
    - https://rtdatavis.github.io/#social-vr-museum
    newUpdates: false
    status: complete
    type: student
    updates: null
- - Spring Break Covid Photo Maps
  - contacts: null
    dates: null
    description: null
    links:
    - https://rtdatavis.github.io/#spring-break-covid
    newUpdates: false
    status: complete
    type: collaboration
    updates: null
- - Tech Core Level Up Presentation Monday, Sept 28 2020
  - contacts: null
    dates: null
    description: null
    links:
    - https://rtdatavis.github.io/#techcoresept28
    newUpdates: false
    status: complete
    type: workshop
    updates: null
- - Tech Core Level Up Presentation Tuesday, Mar 17 2020
  - contacts: null
    dates: null
    description: null
    links:
    - https://rtdatavis.github.io/#techcoremar20
    newUpdates: false
    status: complete
    type: workshop
    updates: null
- - Virtualgl For Nvidia Accelerated Remote Hpc Visualizations
  - contacts: null
    dates: null
    description: null
    links:
    - https://rtdatavis.github.io/#virtualgl
    newUpdates: false
    status: complete
    type: infrastructure
    updates: null
- - 'Womens Hackathon: Visualization On The Web Workshop'
  - contacts: null
    dates: null
    description: null
    links:
    - https://womenshackathon.arizona.edu/
    - https://www.youtube.com/channel/UCe1YiJ53o3qcayVs4cipeXA/videos
    - https://www.youtube.com/watch?v=VLwPOtqW8oM
    newUpdates: false
    status: complete
    type: workshop
    updates:
    - - Taught "Making art on the web with code" workshop one more time
      - Low attendance (4 people)
      - Tested using repl.it collaborative coding platform for teaching, proved a
        bit complex and prone to people accidentally deleting the entire program
      - Facilitated other needs with the discord chat such as sharing the youtube
        links when asked for
    - - Provided support for other workshop presenters
      - Assisted with questions related to presenation methods during the virtual
        hackathon
      - Helped upload a few different presenter's videos to the youtube channel
    - - Recorded my workshop on friday evening
      - Second hour on HTML/CSS started recording but didn't finish so will have to
        re-record and upload this week
      - Trouble shot issues with uploads of longer videos on saturday morning
      - Apparently the email notifications for participants didn't go out so I may
        be teaching this again next saturday
      - Had 4 people attend and taught about basic p5.js as well as how to use it
        to create webpage interfaces that can affect the visualizations
    - - Brainstorming and organizing material for my upcoming pre-recorded workshop
        on creative coding with p5.js
    - - Provided feed back to Jen on the website that has the schedule for the event
- - Xpra And Singularity For Comprehensive Graphical Application Support On Hpc
  - contacts: null
    dates: null
    description: null
    links:
    - https://rtdatavis.github.io/#xprasingularity
    newUpdates: false
    status: complete
    type: infrastructure
    updates: null
- - Meeting With Tyson Swetnam To Discuss Containers And Remote Visualization
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: null
    type: null
    updates:
    - - Had recent break through using different technology, need to reach out to
        Tyson to discuss "NoMachine"
    - - Discussed approaches to using containers for remote visualization
      - Shared videos of my developments with him, waiting to hear feedback
      - Discussed option for teaching small special interest group session on container's
        equipped for remote visualization in the upcoming container bootcamp
      - He mentioned an opportunity to try out some proprietary nvidia remote desktop
        software, but we will only have 45 days to use it so I'm waiting for a less
        busy time of the semester to follow up
      - He was interested in the work of Florian Feldhaus and his xpra virtualgl EGL
        backend running out a docker container as a way to remove the X server dependency
        for accelerated remote visualization
- - Annalysa Lovos Publication Figure Rescaling
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: complete
    type: consult
    updates:
    - - Annalysa was an audience member for the brief presentation that I did with
        the Brain Modeling Workgroup
      - She is working on a neuroimaging paper and needed to improve the resolution
        of some of her figures
      - She had a very tight turn around and didn't reply to the second email offer
        of when to meet but was able to figure out something
      - She is going to reach out if the reviewers ask for anything else
- - ENVS Term paper Consultation on Cu+ exposure and environment resilience
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: complete
    type: collaboration
    updates:
    - - I met with Taylor McCoy this week to help her plan her term paper visualizations
        during my drop in hours
      - I also shared with her the google colab platform introduction for using python
        in a notebook environment without having to perform installations
- - Lilliana  Salas GIS GIDP
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: complete
    type: consult
    updates:
    - - Lilliana is trying to gather her data on alternative transport mechanisms
        in the city of Austin
      - I've been helping her collect data for her publication reviewers and introducing
        the google colab python notebooks for reproducible science
- - Laura Miller Math
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: active
    type: consult
    updates:
    - - Met with Laura to see if she could help me test the Visit program setup for
        the HPC
      - We ran into some issues with version 3.1.1 and she is going to track down
        some better data for our benchmarking
    - - Worked on getting the jellyfish velocity field out of the VTK/paraview format
        so that I could view it in a wider range of tools using Meshio the python
        3d conversion package
    - - Laura has some really neat 3d flow data of what happens in a fluid when a
        JellyFish pulses and I'm trying to help her create some non vtk/visit visualizations
- - Arminda Estrada, ECE
  - contacts: null
    dates: null
    description: null
    links: null
    newUpdates: false
    status: complete
    type: consult
    updates:
    - - Arminda wrote to me and informed me that she was able to get everything setup
        which is great!
    - - Arminda and I worked on trying to get her machine to remotely display the
        ROS open source robotics program with Nvidia Hardware Acceleration
names:
- Streaming technology for HPC
- Vulkan on HPC
- Virtual Nature
- Stellarscape Astronomy Multimedia Dance Performance
- 'Migrant Forensic Empathy project: a Digital Borderlands grant initiative'
- Independent study Abby Collier
- Presentation for Civil Engineering Department
- Data Visualization Roadshow with Jeff Oliver and Kiri Carini
- Judging the data visualization challenge
- Remote visualization infrastructure Development spring 2021
- Oyster Vibrio Literature Review
- Ray tracing on the HPC
- HAS Faculty collaborations with Holodeck
- COVID Retail Mitigation Web Scraping
- Autamus web interface
- Bryan Carter Photogrammetry
- 'Womens Hackathon: Visualization on the web workshop'
- ObservableHQ portfolio of Data Visualization
- Jason Hortin holographic dance graduate project
- Advice for thesis defense visualizations, Sabrina Nardin
- Collaboration with TechCore's Summer Internship
- Resbaz organizer and workshop provider
- force directed biochem networks
- neuro choropleth
- spring break COVID photo maps
- VirtualGL for Nvidia accelerated remote HPC visualizations
- Xpra and Singularity for comprehensive Graphical application support on HPC
- 3D & VR Retrofit AZLIVE
- BIO5 virtual reality tour
- social VR museum capstone student project
- Tech core level up presentation Tuesday, Mar 17 2020
- Tech core level up presentation Monday, Sept 28 2020
- Argonne GPU hackathon
- Radiology 1st year resident Carl Sabotke
- Mt. Lemmon in your Pocket-creating a virtual reality tour
- Meeting with Tyson Swetnam to discuss containers and remote visualization
- IEEE Satellite Vis Conference Planning
- Thermal Imaging Project
- CATalyst data studio vis wall playlist
- Volumetric Capture processing on HPC
- TURN UP Festival Performance NYU/UA collab
